{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO0CddekBj87Wjgbq7oV3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasWong-ST/Intro-to-RL/blob/main/On_Policy_Control_with_Approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B86ienmw52J6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MountainCar:\n",
        "    \"\"\"\n",
        "    Continuous Mountain Car environment\n",
        "    with discrete actions: -1 (reverse), 0 (coast), +1 (forward)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # constants\n",
        "        self.min_position = -1.2\n",
        "        self.max_position = 0.6\n",
        "        self.max_speed = 0.07\n",
        "        self.goal_position = 0.5\n",
        "\n",
        "        self.force = 0.001\n",
        "        self.gravity = 0.0025\n",
        "\n",
        "        self.actions = [-1, 0, 1]\n",
        "\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Randomize initial position and set velocity to 0\"\"\"\n",
        "        position = np.random.uniform(-0.6, -0.4)\n",
        "        velocity = 0.0\n",
        "        self.state = np.array([position, velocity], dtype=np.float32)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Update physics for one time step.\n",
        "        action ∈ {-1, 0, 1}\n",
        "        \"\"\"\n",
        "        position, velocity = self.state\n",
        "\n",
        "        # physics update\n",
        "        velocity += (self.force * action) - (self.gravity * np.cos(3 * position))\n",
        "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
        "        position += velocity\n",
        "        position = np.clip(position, self.min_position, self.max_position)\n",
        "\n",
        "        # if car hits left bound, stop it\n",
        "        if position <= self.min_position and velocity < 0:\n",
        "            velocity = 0.0\n",
        "\n",
        "        # check terminal\n",
        "        done = bool(position >= self.goal_position)\n",
        "\n",
        "        # reward: -1 per step until goal\n",
        "        reward = -1.0\n",
        "\n",
        "        self.state = np.array([position, velocity], dtype=np.float32)\n",
        "        return self.state, reward, done, {}\n"
      ],
      "metadata": {
        "id": "LX2QRhbEFM94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = MountainCar()\n",
        "s = env.reset()\n",
        "for t in range(10):\n",
        "    a = np.random.choice(env.actions)\n",
        "    s_next, r, done, _ = env.step(a)\n",
        "    print(f\"Step {t}: state={s_next}, reward={r}, done={done}\")\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJKX--pQjAvT",
        "outputId": "8c9d8cae-57dd-4f74-d7f5-3d5414d94426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: state=[-0.49008355 -0.00126034], reward=-1.0, done=False\n",
            "Step 1: state=[-0.49259484 -0.00251128], reward=-1.0, done=False\n",
            "Step 2: state=[-0.49533832 -0.00274348], reward=-1.0, done=False\n",
            "Step 3: state=[-0.4972935  -0.00195518], reward=-1.0, done=False\n",
            "Step 4: state=[-0.49944577 -0.00215226], reward=-1.0, done=False\n",
            "Step 5: state=[-0.50077903 -0.00133325], reward=-1.0, done=False\n",
            "Step 6: state=[-0.5022833  -0.00150427], reward=-1.0, done=False\n",
            "Step 7: state=[-0.5049473  -0.00266402], reward=-1.0, done=False\n",
            "Step 8: state=[-0.50875115 -0.00380384], reward=-1.0, done=False\n",
            "Step 9: state=[-0.51366633 -0.00491516], reward=-1.0, done=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numtilings = 8\n",
        "iht = IHT(4096)\n",
        "get_tiles(0, 1, 1)\n",
        "\n",
        "\"\"\"\n",
        "Each integer in this list is the index corresponding\n",
        "to the specific tile the state [-0.5, 0.0] falls into\n",
        "within each of the 8 tilings, considering the action 1.\n",
        "This list of 8 indices is the feature representation\n",
        "x(s, a) for that specific state-action pair.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjEoBcvhsfz1",
        "outputId": "9ede8651-50d4-4944-9e9a-13eb22384b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method MountainCar.reset of <__main__.MountainCar object at 0x7a0f645d09e0>>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tiles(position, velocity, action):\n",
        "  \"\"\"\n",
        "  Calculates the active tile indices for a given state and action.\n",
        "  \"\"\"\n",
        "  # Scale the continuous state variables (as discussed)\n",
        "  min_pos, max_pos = -1.2, 0.6\n",
        "  min_vel, max_vel = -0.07, 0.07\n",
        "\n",
        "  scaled_pos = (position - min_pos) / (max_pos - min_pos) * numtilings\n",
        "  scaled_vel = (velocity - min_vel) / (max_vel - min_vel) * numtilings\n",
        "\n",
        "  # The 'tiles' function expects lists for floats and ints\n",
        "  float_features = [scaled_pos, scaled_vel]\n",
        "  int_features = [action] # Include the discrete action\n",
        "\n",
        "  # Get the indices of the active tiles from the tile coder\n",
        "  # We pass iht, numtilings, the scaled floats, and the action as an int feature\n",
        "  active_tiles = tiles(iht, numtilings, float_features, int_features)\n",
        "\n",
        "  return active_tiles\n",
        "\n",
        "def get_q_value(state_tiles, w):\n",
        "  \"\"\"\n",
        "  Calculates q_hat(s, a, w) by summing weights for active tiles.\n",
        "  state_tiles: The list/array of active tile indices for a state-action pair.\n",
        "  w: The weight vector.\n",
        "  \"\"\"\n",
        "  # The value is the sum of weights corresponding to active tiles\n",
        "  return np.sum(w[state_tiles])\n",
        "\n",
        "def choose_action(position, velocity, w, epsilon=0.1):\n",
        "  \"\"\"\n",
        "  Chooses an action using an epsilon-greedy policy.\n",
        "  \"\"\"\n",
        "  possible_actions = [-1, 0, 1]\n",
        "  q_values = []\n",
        "  for action in possible_actions:\n",
        "    tiles_for_action = get_tiles(position, velocity, action)\n",
        "    q_values.append(get_q_value(tiles_for_action, w))\n",
        "\n",
        "  if np.random.rand() < epsilon:\n",
        "    # Exploration: choose a random action\n",
        "    return np.random.choice(possible_actions)\n",
        "  else:\n",
        "    # Exploitation: choose the action with the highest q-value\n",
        "    best_action_index = np.argmax(q_values)\n",
        "    return possible_actions[best_action_index]"
      ],
      "metadata": {
        "id": "ddnPJtacluaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Size: The vector x(s, a) has a large number of components, one for every possible tile index (4096 in our example).\n",
        "\n",
        "Activation: For a specific state s and action a, the get_tiles function tells us which few tiles are \"active\" (e.g., it gives us a list of 8 indices).\n",
        "\n",
        "Binary & Sparse: The feature vector x(s, a) is binary (contains only 0s and 1s) and sparse (mostly 0s).\n",
        "\n",
        "It has a 1 at the component corresponding to each active tile index found by get_tiles.\n",
        "\n",
        "It has a 0 at all other components.\n",
        "\n",
        "So, for any given state-action pair, x(s, a) is a long vector with exactly numtilings (8 in our case) components equal to 1,\n",
        "and all the rest (4096 - 8 = 4088) equal to 0. This sparsity is what makes calculating the action value q̂(s, a, w) efficient.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WIdJGuPno_3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semi_gradient_sarsa_mountain_car(env, num_episodes=1000, max_steps_per_episode=500,\n",
        "                                    gamma=1.0, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Implements Episodic Semi-gradient Sarsa for MountainCar-v0 using Tile Coding.\n",
        "    alpha: Step size, often scaled by the number of active features (tilings).\n",
        "    \"\"\"\n",
        "    w = np.zeros(4096) # Initialize weights to zero\n",
        "    numtilings = 8\n",
        "    alpha=0.1/numtilings\n",
        "\n",
        "    episode_rewards = [] # To track learning progress\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        position, velocity = state\n",
        "        action = choose_action(position, velocity, w, epsilon)\n",
        "        active_tiles_current = get_tiles(position, velocity, action)\n",
        "\n",
        "        current_episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while steps < max_steps_per_episode:\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_position, next_velocity = next_state\n",
        "            current_episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                # Calculate TD error for terminal state (next q-value is 0)\n",
        "                q_value_current = get_q_value(active_tiles_current, w)\n",
        "                td_error = reward - q_value_current\n",
        "\n",
        "                # Update weights for the last state-action pair\n",
        "                w[active_tiles_current] += alpha * td_error\n",
        "                # No gradient calculation needed as it's just 1 for active tiles\n",
        "                break # End episode\n",
        "                print(steps)\n",
        "\n",
        "            # Choose the next action A' based on the next state S'\n",
        "            next_action = choose_action(next_position, next_velocity, w, epsilon)\n",
        "            active_tiles_next = get_tiles(next_position, next_velocity, next_action)\n",
        "\n",
        "            # Calculate TD error for non-terminal state\n",
        "            q_value_current = get_q_value(active_tiles_current, w)\n",
        "            q_value_next = get_q_value(active_tiles_next, w)\n",
        "            td_target = reward + gamma * q_value_next\n",
        "            td_error = td_target - q_value_current\n",
        "\n",
        "            # Update weights w using the active tiles for the *current* state-action pair\n",
        "            # The gradient is implicitly handled by only updating active tiles\n",
        "            w[active_tiles_current] += alpha * td_error\n",
        "\n",
        "            # Move to the next state and action\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            active_tiles_current = active_tiles_next # Use the tiles for the *next* step\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "        episode_rewards.append(current_episode_reward)\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Episode {ep + 1}/{num_episodes} completed. Average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "\n",
        "    return w, episode_rewards\n",
        "\n",
        "def generate_episode_with_learned_policy(env, w):\n",
        "    \"\"\"\n",
        "    Generates an episode using the greedy policy derived from weights w,\n",
        "    and returns the trajectory and number of steps.\n",
        "    \"\"\"\n",
        "    episode_trajectory = [] # List to store (state, action, reward) tuples\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    max_steps_eval = 1000 # Put a limit to prevent infinite loops if policy is bad\n",
        "\n",
        "    while not done and steps < max_steps_eval:\n",
        "        position, velocity = state\n",
        "        # Choose action greedily (epsilon = 0)\n",
        "        action = choose_action(position, velocity, w, epsilon=0)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_trajectory.append({\"state\": state, \"action\": action, \"reward\": reward})\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "    return episode_trajectory, steps"
      ],
      "metadata": {
        "id": "j9vfJ12zqL4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = MountainCar()\n",
        "numtilings = 8\n",
        "iht = IHT(4096)\n",
        "w, rewards = semi_gradient_sarsa_mountain_car(env, num_episodes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKjxpnSJuFua",
        "outputId": "3782a576-4848-411c-f4cc-d303cdc08fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100/1000 completed. Average reward (last 100): -340.73\n",
            "Episode 200/1000 completed. Average reward (last 100): -196.95\n",
            "Episode 300/1000 completed. Average reward (last 100): -163.51\n",
            "Episode 400/1000 completed. Average reward (last 100): -149.48\n",
            "Episode 500/1000 completed. Average reward (last 100): -147.48\n",
            "Episode 600/1000 completed. Average reward (last 100): -139.59\n",
            "Episode 700/1000 completed. Average reward (last 100): -139.57\n",
            "Episode 800/1000 completed. Average reward (last 100): -141.89\n",
            "Episode 900/1000 completed. Average reward (last 100): -131.72\n",
            "Episode 1000/1000 completed. Average reward (last 100): -125.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_episode_with_learned_policy(env, w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZOJhAFMRvb7m",
        "outputId": "40e0a8bf-ecb5-41c1-92bb-ed279cc71b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'state': array([-0.47911423,  0.        ], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.48044688, -0.00133264], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.48310226, -0.00265538], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.4870606 , -0.00395836], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.49229246, -0.00523185], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.49875876, -0.0064663 ], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5064112 , -0.00765243], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.51519245, -0.00878128], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.52503675, -0.00984432], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5358703 , -0.01083353], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.54761183, -0.01174152], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5601734 , -0.01256157], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.57346123, -0.01328781], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5873765 , -0.01391524], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6018163 , -0.01443982], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.61667484, -0.01485856], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.63184434, -0.01516952], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6472162 , -0.01537187], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.66268206, -0.01546585], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6781348 , -0.01545273], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6934696 , -0.01533478], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7085848 , -0.01511519], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7233827 , -0.01479792], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7377704 , -0.01438765], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.75166   , -0.01388962], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7649695 , -0.01330952], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7776229 , -0.01265335], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7895502 , -0.01192729], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.80068785, -0.01113764], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.81097853, -0.01029068], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8203711 , -0.00939259], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8288205 , -0.00844941], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8342875 , -0.00546699], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8367474 , -0.00245986], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-8.3618921e-01,  5.5822264e-04], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8326154 ,  0.00357383], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.82604194,  0.00657346], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8164988 ,  0.00954311], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.80403095,  0.01246789], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.78869927,  0.01533166], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.77058244,  0.01811685], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7497781 ,  0.02080432], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7264046 ,  0.02337346], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.70060223,  0.02580239], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6725338,  0.0280684], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6423852 ,  0.03014861], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.61036444,  0.03202077], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.57670027,  0.03366419], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5416395 ,  0.03506077], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5054435 ,  0.03619601], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.4683836 ,  0.03705991], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.43073592,  0.03764769], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.3927757 ,  0.03796019], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.3547717 ,  0.03800401], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.31698045,  0.03779125], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.27964148,  0.03733896], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.24297318,  0.0366683 ], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.2071696 ,  0.03580357], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.17239854,  0.03477107], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.1388005 ,  0.03359804], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.10648884,  0.03231167], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.07555068,  0.03093816], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.04604857,  0.0295021 ], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.01802266,  0.02802592], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.00850691, 0.02652957], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.0335373 , 0.02503039], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.05708033, 0.02354303], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.07915992, 0.02207959], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.09980968, 0.02064976], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.11907067, 0.019261  ], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.13698947, 0.01791881], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.15361644, 0.01662697], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.16900423, 0.01538778], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.18320651, 0.01420229], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.196277  , 0.01307048], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.2082685, 0.0119915], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.2192323 , 0.01096381], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.2292176, 0.0099853], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.23827106, 0.00905346], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.24643648, 0.00816542], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.25375456, 0.00731808], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.26026273, 0.00650817], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.265995  , 0.00573227], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.2709819, 0.0049869], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.2752504 , 0.00426849], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.27682388, 0.00157348], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.27571103, -0.00111284], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.27190572, -0.00380531], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.26538706, -0.00651868], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.25611976, -0.00926731], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.24405481, -0.01206494], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.22913054, -0.01492427], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.21127401, -0.01785653], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.19040306, -0.02087095], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.16642898, -0.02397407], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.1392601 , -0.02716888], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.10980624, -0.02945386], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.07798681, -0.03181944], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.04373548, -0.03425132], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([ 0.00700565, -0.03672983], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.03222363, -0.03922928], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.07394125, -0.04171761], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.1190976 , -0.04515635], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.16759607, -0.04849847], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.21928515, -0.05168908], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.27395248, -0.05466734], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.33132198, -0.05736949], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.3910549 , -0.05973292], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.4527559 , -0.06170101], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.51598424, -0.06322834], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5802697 , -0.06428544], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6451321 , -0.06486245], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.71010315, -0.06497101], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.77474725, -0.06464408], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8386809 , -0.06393369], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.901588  , -0.06290709], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.9632299 , -0.06164185], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.0234506 , -0.06022074], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.0821776 , -0.05872709], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.1394185 , -0.05724084], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.1932544 , -0.05383591], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.2,  0. ], dtype=float32), 'action': 1, 'reward': -1.0},\n",
              "  {'state': array([-1.1967582,  0.0032419], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.1902637 ,  0.00649445], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.1804961 ,  0.00976769], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.1674256 ,  0.01307045], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.151016  ,  0.01640959], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.1312267 ,  0.01978931], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.1080164 ,  0.02321029], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.0813476 ,  0.02666879], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.0511919 ,  0.03015568], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-1.0175364 ,  0.03365551], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.9803908 ,  0.03714561], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.9397952 ,  0.04059557], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.89582825,  0.04396692], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8496147 ,  0.04621355], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.8003276 ,  0.04928707], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.7481954 ,  0.05213222], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.6935033,  0.0546921], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.63659143,  0.05691192], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5778482 ,  0.05874322], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.5176999,  0.0601483], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.45659584,  0.06110406], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.39499092,  0.06160491], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.33332682,  0.06166411], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.27201352,  0.06131331], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.21141298,  0.06060053], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.15182626,  0.05958673], dtype=float32),\n",
              "   'action': -1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.09548466,  0.0563416 ], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([-0.04054119,  0.05494347], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.01292074, 0.05346194], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.06488456, 0.05196381], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.11539559, 0.05051103], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.16355494, 0.04815935], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.20950924, 0.0459543 ], dtype=float32),\n",
              "   'action': 0,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.2534413 , 0.04393206], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.29656184, 0.04312053], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.33910823, 0.04254639], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.3813405 , 0.04223228], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.42353794, 0.04219744], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0},\n",
              "  {'state': array([0.46599615, 0.0424582 ], dtype=float32),\n",
              "   'action': 1,\n",
              "   'reward': -1.0}],\n",
              " 161)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tile Coding"
      ],
      "metadata": {
        "id": "QKDXOkIXy_11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basehash = hash\n",
        "\n",
        "class IHT:\n",
        "    \"Structure to handle collisions\"\n",
        "    def __init__(self, sizeval):\n",
        "        self.size = sizeval\n",
        "        self.overfullCount = 0\n",
        "        self.dictionary = {}\n",
        "\n",
        "    def __str__(self):\n",
        "        \"Prepares a string for printing whenever this object is printed\"\n",
        "        return \"Collision table:\" + \\\n",
        "               \" size:\" + str(self.size) + \\\n",
        "               \" overfullCount:\" + str(self.overfullCount) + \\\n",
        "               \" dictionary:\" + str(len(self.dictionary)) + \" items\"\n",
        "\n",
        "    def count (self):\n",
        "        return len(self.dictionary)\n",
        "\n",
        "    def fullp (self):\n",
        "        return len(self.dictionary) >= self.size\n",
        "\n",
        "    def getindex (self, obj, readonly=False):\n",
        "        d = self.dictionary\n",
        "        if obj in d: return d[obj]\n",
        "        elif readonly: return None\n",
        "        size = self.size\n",
        "        count = self.count()\n",
        "        if count >= size:\n",
        "            if self.overfullCount==0: print('IHT full, starting to allow collisions')\n",
        "            self.overfullCount += 1\n",
        "            return basehash(obj) % self.size\n",
        "        else:\n",
        "            d[obj] = count\n",
        "            return count\n",
        "\n",
        "def hashcoords(coordinates, m, readonly=False):\n",
        "    if type(m)==IHT: return m.getindex(tuple(coordinates), readonly)\n",
        "    if type(m)==int: return basehash(tuple(coordinates)) % m\n",
        "    if m==None: return coordinates\n",
        "\n",
        "from math import floor, log\n",
        "from itertools import zip_longest\n",
        "\n",
        "def tiles (ihtORsize, numtilings, floats, ints=[], readonly=False):\n",
        "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
        "    qfloats = [floor(f*numtilings) for f in floats]\n",
        "    Tiles = []\n",
        "    for tiling in range(numtilings):\n",
        "        tilingX2 = tiling*2\n",
        "        coords = [tiling]\n",
        "        b = tiling\n",
        "        for q in qfloats:\n",
        "            coords.append( (q + b) // numtilings )\n",
        "            b += tilingX2\n",
        "        coords.extend(ints)\n",
        "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
        "    return Tiles\n",
        "\n",
        "def tileswrap (ihtORsize, numtilings, floats, wrapwidths, ints=[], readonly=False):\n",
        "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints, wrapping some floats\"\"\"\n",
        "    qfloats = [floor(f*numtilings) for f in floats]\n",
        "    Tiles = []\n",
        "    for tiling in range(numtilings):\n",
        "        tilingX2 = tiling*2\n",
        "        coords = [tiling]\n",
        "        b = tiling\n",
        "        for q, width in zip_longest(qfloats, wrapwidths):\n",
        "            c = (q + b%numtilings) // numtilings\n",
        "            coords.append(c%width if width else c)\n",
        "            b += tilingX2\n",
        "        coords.extend(ints)\n",
        "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
        "    return Tiles"
      ],
      "metadata": {
        "id": "HPXZAhMozCkF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}