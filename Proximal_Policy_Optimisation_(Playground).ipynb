{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIFcnTNsFkbvyFHg8JxnVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasWong-ST/Intro-to-RL/blob/main/Proximal_Policy_Optimisation_(Playground).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Proximal Policy Optimization Algorithm\n",
        "We consider a stochastic policy $\\pi_\\theta(a \\mid s)$ with parameters $\\theta$\n",
        "and a value function $V_w(s)$ with parameters $w$. Given a batch of transitions\n",
        "$\\{(s_t, a_t, r_{t+1}, s_{t+1})\\}_{t=1}^N$ collected under an old policy\n",
        "$\\pi_{\\theta_{\\text{old}}}$, the PPO objective is\n",
        "\n",
        "$$\n",
        "J(\\theta, w)\n",
        "=\n",
        "\\mathbb{E}_t \\Big[\n",
        "  L_t^{\\text{CLIP}}(\\theta)\n",
        "  - c_1 L_t^{\\text{VF}}(w)\n",
        "  + c_2 S[\\pi_\\theta](s_t)\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "1. **Probability ratio**\n",
        "\n",
        "   $$\n",
        "   r_t(\\theta)\n",
        "   = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}.\n",
        "   $$\n",
        "\n",
        "2. **Clipped policy (actor) loss**\n",
        "\n",
        "   $$\n",
        "   L_t^{\\text{CLIP}}(\\theta)\n",
        "   =\n",
        "   \\min\\Big(\n",
        "     r_t(\\theta)\\,\\hat A_t,\\;\n",
        "     \\operatorname{clip}\\big(r_t(\\theta), 1 - \\varepsilon, 1 + \\varepsilon\\big)\n",
        "     \\, \\hat A_t\n",
        "   \\Big).\n",
        "   $$\n",
        "\n",
        "   Here $\\hat A_t$ is an estimator of the advantage function, e.g. from\n",
        "   Generalized Advantage Estimation (GAE).\n",
        "\n",
        "3. **Value (critic) loss**\n",
        "\n",
        "   $$\n",
        "   L_t^{\\text{VF}}(w)\n",
        "   =\n",
        "   \\big(\n",
        "     V_w(s_t) - \\hat V_t^{\\text{targ}}\n",
        "   \\big)^2.\n",
        "   $$\n",
        "\n",
        "   Here $\\hat V_t^{\\text{targ}}$ is a target for $v^\\pi(s_t)$, for example\n",
        "   $$\n",
        "   \\hat V_t^{\\text{targ}} = \\hat A_t + V_{w_{\\text{old}}}(s_t),\n",
        "   $$\n",
        "   where $V_{w_{\\text{old}}}$ is the value function used to compute advantages.\n",
        "\n",
        "4. **Entropy bonus (for discrete actions)**\n",
        "\n",
        "   $$\n",
        "   S[\\pi_\\theta](s_t)\n",
        "   =\n",
        "   - \\sum_{a} \\pi_\\theta(a \\mid s_t)\n",
        "         \\log \\pi_\\theta(a \\mid s_t).\n",
        "   $$\n",
        "\n",
        "The full loss used for gradient *descent* is typically\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta, w)\n",
        "=\n",
        "- \\mathbb{E}_t\\big[ L_t^{\\text{CLIP}}(\\theta) \\big]\n",
        "+ c_1 \\mathbb{E}_t\\big[ L_t^{\\text{VF}}(w) \\big]\n",
        "- c_2 \\mathbb{E}_t\\big[ S[\\pi_\\theta](s_t) \\big],\n",
        "$$\n",
        "\n",
        "where $c_1 \\ge 0$ controls the strength of the value loss,\n",
        "and $c_2 \\ge 0$ controls the strength of the entropy regularization.\n"
      ],
      "metadata": {
        "id": "heUoFLXjfwD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPO â€“ Training Pipeline (Summary)\n",
        "\n",
        "We use two neural networks:\n",
        "\n",
        "- Actor (policy): $\\pi_\\theta(a \\mid s)$ with parameters $\\theta$  \n",
        "- Critic (value): $V_w(s)$ with parameters $w$\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. Initialize\n",
        "\n",
        "- Initialize $\\theta$ and $w$\n",
        "- Set $\\theta_{\\text{old}} \\leftarrow \\theta$, $w_{\\text{old}} \\leftarrow w$\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Rollout (data collection with old parameters)\n",
        "\n",
        "Using the **old** policy and value function:\n",
        "\n",
        "- For $t = 0, 1, \\dots, T-1$:\n",
        "  - Observe state $s_t$\n",
        "  - Sample action from old policy:\n",
        "    $$\n",
        "    a_t \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot \\mid s_t)\n",
        "    $$\n",
        "  - Execute $a_t$, observe reward $r_{t+1}$ and next state $s_{t+1}$\n",
        "  - Store:\n",
        "    - $s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$, done flag\n",
        "    - $\\log \\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)$\n",
        "    - $V_{w_{\\text{old}}}(s_t)$\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Compute advantages and value targets (with old critic)\n",
        "\n",
        "Using the **old** value network $V_{w_{\\text{old}}}$:\n",
        "\n",
        "- For each $t$, compute value:\n",
        "  $$\n",
        "  v_t = V_{w_{\\text{old}}}(s_t)\n",
        "  $$\n",
        "- Define TD-errors (for GAE):\n",
        "  $$\n",
        "  \\delta_t = r_{t+1} + \\gamma V_{w_{\\text{old}}}(s_{t+1}) - V_{w_{\\text{old}}}(s_t)\n",
        "  $$\n",
        "- Generalized Advantage Estimation (backwards over the trajectory):\n",
        "  $$\n",
        "  \\hat A_t = \\delta_t + \\gamma \\lambda \\hat A_{t+1}\n",
        "  $$\n",
        "  (with $\\hat A_T = 0$ at terminal)\n",
        "\n",
        "- Value targets (for critic):\n",
        "  $$\n",
        "  \\hat V_t^{\\text{targ}} = \\hat A_t + V_{w_{\\text{old}}}(s_t)\n",
        "  $$\n",
        "\n",
        "Optionally normalize advantages:\n",
        "$$\n",
        "\\hat A_t \\leftarrow\n",
        "\\frac{\\hat A_t - \\text{mean}(\\hat A)}{\\text{std}(\\hat A) + \\epsilon}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. PPO update (multiple epochs over the same batch)\n",
        "\n",
        "Set **trainable** parameters:\n",
        "$$\n",
        "\\theta \\leftarrow \\theta_{\\text{old}}, \\quad w \\leftarrow w_{\\text{old}}\n",
        "$$\n",
        "\n",
        "Repeat for several epochs and mini-batches:\n",
        "\n",
        "1. **Policy ratio**\n",
        "   $$\n",
        "   r_t(\\theta) =\n",
        "   \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}\n",
        "   $$\n",
        "\n",
        "2. **Clipped policy loss**\n",
        "   $$\n",
        "   L_t^{\\text{CLIP}}(\\theta) =\n",
        "   \\min\\Big(\n",
        "     r_t(\\theta)\\,\\hat A_t,\\;\n",
        "     \\text{clip}(r_t(\\theta), 1-\\varepsilon, 1+\\varepsilon)\\,\\hat A_t\n",
        "   \\Big)\n",
        "   $$\n",
        "\n",
        "3. **Value (critic) loss**\n",
        "   $$\n",
        "   L_t^{\\text{VF}}(w) =\n",
        "   \\big( V_w(s_t) - \\hat V_t^{\\text{targ}} \\big)^2\n",
        "   $$\n",
        "\n",
        "   (Optionally with value clipping, using $V_{w_{\\text{old}}}(s_t)$.)\n",
        "\n",
        "4. **Entropy bonus (discrete actions)**\n",
        "   $$\n",
        "   S[\\pi_\\theta](s_t) =\n",
        "   - \\sum_a \\pi_\\theta(a \\mid s_t)\\,\\log \\pi_\\theta(a \\mid s_t)\n",
        "   $$\n",
        "\n",
        "5. **Total objective (to maximize)**\n",
        "   $$\n",
        "   J(\\theta, w) =\n",
        "   \\mathbb{E}_t\\Big[\n",
        "     L_t^{\\text{CLIP}}(\\theta)\n",
        "     - c_1 L_t^{\\text{VF}}(w)\n",
        "     + c_2 S[\\pi_\\theta](s_t)\n",
        "   \\Big]\n",
        "   $$\n",
        "\n",
        "   In code we usually minimize the **loss**:\n",
        "   $$\n",
        "   \\mathcal{L}(\\theta, w) =\n",
        "   - \\mathbb{E}_t[L_t^{\\text{CLIP}}(\\theta)]\n",
        "   + c_1 \\mathbb{E}_t[L_t^{\\text{VF}}(w)]\n",
        "   - c_2 \\mathbb{E}_t[S[\\pi_\\theta](s_t)]\n",
        "   $$\n",
        "\n",
        "6. **Gradient step**\n",
        "   - Update $\\theta$ and $w$ by gradient descent on $\\mathcal{L}(\\theta, w)$.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Update old parameters\n",
        "\n",
        "After finishing PPO epochs on this batch:\n",
        "\n",
        "$$\n",
        "\\theta_{\\text{old}} \\leftarrow \\theta, \\quad\n",
        "w_{\\text{old}} \\leftarrow w\n",
        "$$\n",
        "\n",
        "Then go back to Step 2 and collect a new batch with the updated policy.\n"
      ],
      "metadata": {
        "id": "vWPERD_TBKMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "w6Y7pB6SfYu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kappa_alpha, sigma_alpha = 5.0, 1.0\n",
        "#kappa_xi, sigma_xi = 15.0, 100.0\n",
        "\n",
        "#alpha[i+1] = alpha[i] - kappa_alpha*alpha[i]*dt + sigma_alpha * np.sqrt(dt) * np.random.randn()\n",
        "#xi[i+1]    = xi[i]    - kappa_xi   *xi[i]*dt + sigma_xi    * np.sqrt(dt) * np.random.randn()"
      ],
      "metadata": {
        "id": "O3BfYhyPwRwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NashBrokerTrader:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "      # 1. Time Parameters\n",
        "        self.T = 1.0        # Total time (e.g., 1 day)\n",
        "        self.dt = 0.01      # Step size\n",
        "        self.num_steps = int(self.T / self.dt)\n",
        "\n",
        "        # 2. OU Process Parameters (Alpha and Xi)\n",
        "        # (Fill these in based on your previous message)\n",
        "        self.kappa_alpha = 5.0\n",
        "        self.sigma_alpha = 1.0\n",
        "        self.kappa_xi = 15.0\n",
        "        self.sigma_xi = 100.0\n",
        "        self.sigma_s = 0.5\n",
        "        self.a = 0.1\n",
        "        self.c = 0.1\n",
        "\n",
        "        # 3. Market Impact Parameters (for dY_t = (h*nu - p*Y)*dt)\n",
        "        self.h = 1e-3   # Temporary impact coefficient\n",
        "        self.p = 0.5   # Impact decay rate\n",
        "\n",
        "        # 4. Reward Parameters\n",
        "        self.phi = 1 # Running inventory penalty\n",
        "\n",
        "        # 5. State variables (Placeholder for now)\n",
        "        self.t = 0\n",
        "        # We will define the state vector in reset()\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "      \"\"\"Resets the environment to the starting state.\"\"\"\n",
        "      self.t = 0 # start back at initial time\n",
        "      self.Q_b, self.Y_impact, self.alpha, self.xi_uninformed = 0, 0, 0, 0\n",
        "      # Return state as a NumPy array (for compatibility with PyTorch later)\n",
        "      return np.array([self.Q_b, self.Y_impact, self.alpha, self.xi_uninformed], dtype=np.float32)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "      nu = action\n",
        "\n",
        "      # --- 1. Calculate Changes (Deltas) based on CURRENT State (t) ---\n",
        "\n",
        "      # Alpha & Xi (OU Processes)\n",
        "      d_alpha = -self.kappa_alpha * self.alpha * self.dt + self.sigma_alpha * np.sqrt(self.dt) * np.random.randn()\n",
        "\n",
        "      d_xi = -self.kappa_xi * self.xi_uninformed * self.dt + self.sigma_xi * np.sqrt(self.dt) * np.random.randn()\n",
        "\n",
        "      # Inventory Change (dQ)\n",
        "      d_Q = (nu - self.xi_uninformed) * self.dt\n",
        "\n",
        "      # Impact Change (dY)\n",
        "      d_Y = (self.h * nu - self.p * self.Y_impact) * self.dt\n",
        "\n",
        "      # Price Change (dS)\n",
        "      # dS = alpha*dt + dY + noise, noise = sigma * browian_motion**2\n",
        "      price_noise = self.sigma_s * np.sqrt(self.dt) * np.random.randn()\n",
        "      d_S = (self.alpha * self.dt) + d_Y + price_noise\n",
        "\n",
        "      # --- 2. Calculate Reward (Based on t and dt) ---\n",
        "\n",
        "      # PnL from Trading (Profit from spread - Cost of trading)\n",
        "      execution_pnl = (self.c * self.xi_uninformed**2 - self.a * nu**2) * self.dt\n",
        "\n",
        "      # PnL from Holding Inventory (Mark-to-Market change)\n",
        "      inventory_pnl = self.Q_b * d_S\n",
        "\n",
        "      # Penalty for Risk (Urgency)\n",
        "      risk_penalty = -self.phi * (self.Q_b**2) * self.dt\n",
        "\n",
        "      reward = execution_pnl + inventory_pnl + risk_penalty\n",
        "\n",
        "      # --- 3. Update State to (t + dt) ---\n",
        "      self.t += self.dt\n",
        "      self.alpha += d_alpha\n",
        "      self.xi_uninformed += d_xi\n",
        "      self.Q_b += d_Q\n",
        "      self.Y_impact += d_Y\n",
        "\n",
        "      # --- 4. Return Step Info ---\n",
        "      # State Vector: [Inventory, Impact, Alpha, Flow]\n",
        "      next_state = np.array([self.Q_b, self.Y_impact, self.alpha, self.xi_uninformed], dtype=np.float32)\n",
        "\n",
        "      # Check termination (e.g., time is up)\n",
        "      terminated = self.t >= self.T\n",
        "\n",
        "      return next_state, reward, terminated, False, {}"
      ],
      "metadata": {
        "id": "PAbT16rev0KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Continous Normally Distributed Action Space\n",
        "### Part 1: The Forward Pass (Making a Decision)\n",
        "\n",
        "1. **The Environment** presents a **State $x$** (Input)\n",
        "   * *Example:* `[High Inventory, Price Dropping]`\n",
        "   \n",
        "   $\\\\downarrow$\n",
        "\n",
        "2. **The Neural Network** processes $x$ to calculate a **Mean $\\\\mu$**\n",
        "   * It uses its current weights to decide the \"best guess\" action.\n",
        "   * *Result:* `mu = -3.5` (\"I think we should sell.\")\n",
        "   \n",
        "   $\\\\downarrow$\n",
        "\n",
        "3. **The Parameter `log_std`** provides the **Noise Level $\\\\sigma$**\n",
        "   * This ignores the state $x$. It just asks: \"How confident are we globally?\"\n",
        "   * *Result:* `sigma = 1.0` (\"But I'm willing to explore $\\\\pm 1.0$.\")\n",
        "   \n",
        "   $\\\\downarrow$\n",
        "\n",
        "4. **PyTorch** combines $\\\\mu$ and $\\\\sigma$ into a **Normal Distribution**\n",
        "   * *Result:* A Bell Curve centered at -3.5 with width 1.0.\n",
        "   \n",
        "   $\\\\downarrow$\n",
        "\n",
        "5. **The Agent** samples from this distribution to get the **Action $a$**\n",
        "   * *Result:* `action = -4.2` (A bit more aggressive than the mean).\n",
        "\n",
        "---\n",
        "\n",
        "### Part 2: The Backward Pass (Learning)\n",
        "\n",
        "6. **The Environment** returns a **Reward** and **Advantage $A$**\n",
        "   * *Scenario:* The action `-4.2` was excellent! It cleared inventory and made a profit.\n",
        "   * *Result:* **Positive Advantage ($A > 0$)**.\n",
        "   \n",
        "   $\\\\downarrow$\n",
        "\n",
        "7. **The Loss Function** compares the Action `-4.2` to the Distribution\n",
        "   * It sees that `-4.2` was somewhat far from the center `-3.5`.\n",
        "   * *Goal:* \"Make `-4.2` more likely next time!\"\n",
        "   \n",
        "   $\\\\downarrow$\n",
        "\n",
        "8. **The Optimizer** updates the **Network Weights** (shifting $\\\\mu$)\n",
        "   * It adjusts the weights so that next time $x$ appears, $\\\\mu$ will be closer to `-4.2` (e.g., shifts to `-3.8`).\n",
        "   * *Effect:* The \"bullseye\" moves toward the successful action.\n",
        "   \n",
        "   $\\\\downarrow$\n",
        "\n",
        "9. **The Optimizer** updates **`log_std`** (shifting $\\\\sigma$)\n",
        "   * It sees that the successful action was in the \"tail\" of the curve. To make it more likely, it needs to widen the curve.\n",
        "   * *Effect:* It increases `log_std` slightly, making the agent more willing to explore next time."
      ],
      "metadata": {
        "id": "iISwcy1Oq1eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent_actor(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim = 64):\n",
        "\n",
        "    super(PPOAgent_actor, self).__init__()\n",
        "\n",
        "    # 1. The Mean Network (Outputs mu)\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dim),\n",
        "        nn.Tanh(),  # Tanh is often better for PPO than ReLU\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(hidden_dim, output_dim)\n",
        "    )\n",
        "\n",
        "    # 2. The Learnable Log Std (Exploration Noise)\n",
        "    # We use log_std so we can take exp() later to ensure std is always positive\n",
        "\n",
        "    '''Later, when you define your optimizer:\n",
        "    The optimizer looks at that list and sees the weights of your Linear layers,\n",
        "    the biases of your Linear layers and self.log_std! It treats log_std exactly\n",
        "    the same as a weight in a neural network layer. It is just a \"weight\"\n",
        "    that isn't connected to any input neurons.'''\n",
        "\n",
        "    self.log_std = nn.Parameter(torch.zeros(1, output_dim))\n",
        "\n",
        "  def forward(self, x): # x = [Q^b_t, Y_t, alpha_t, xi_t]\n",
        "    # Return the Mean\n",
        "    mu = self.network(x)\n",
        "    return mu\n",
        "\n",
        "  def get_dist(self, x):\n",
        "    # Helper to get the distribution object\n",
        "    mu = self.forward(x)\n",
        "    sigma = torch.exp(self.log_std.expand_as(mu))\n",
        "    return torch.distributions.Normal(mu, sigma)\n",
        "\n",
        "\n",
        "class PPOAgent_critic(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim = 64):\n",
        "\n",
        "    super(PPOAgent_critic, self).__init__()\n",
        "\n",
        "    # 1. The Value Network\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dim),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(hidden_dim, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hl2ZUxYkn3M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutBuffer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []       # NEW: Needed to handle episode endings\n",
        "        self.values = []      # NEW: The Critic's prediction V(s)\n",
        "\n",
        "    def add(self, state, action, log_prob, reward, done, value):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.values.append(value)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.values = []\n",
        "\n",
        "    def compute_gae(self, last_value, gamma=0.99, lamda=0.95):\n",
        "        # 1. Setup containers\n",
        "        advantages = []\n",
        "        next_value = last_value    # V(s_T+1) (The value of the state after the episode ends)\n",
        "        next_advantage = 0         # A_T+1 (Always 0 by definition)\n",
        "\n",
        "        # 2. The Backward Loop\n",
        "        for i in reversed(range(len(self.rewards))):\n",
        "            # If the episode ended here (done=1), we shouldn't look into the future.\n",
        "            # mask becomes 0 if done, 1 otherwise.\n",
        "            mask = 1 - self.dones[i]\n",
        "\n",
        "            # Calculate Delta (The 1-step TD Error)\n",
        "            # delta = r + gamma * V(next) - V(current)\n",
        "            delta = self.rewards[i] + gamma * next_value * mask - self.values[i]\n",
        "\n",
        "            # Calculate Advantage (The recursive magic)\n",
        "            # A_t = delta + gamma * lambda * A_t+1\n",
        "            advantage = delta + gamma * lamda * next_advantage * mask\n",
        "\n",
        "            # Store it (We interpret it in reverse order later)\n",
        "            '''insert(0, val) pushes the new value to the front of the list,\n",
        "            automatically reversing it back to the correct order'''\n",
        "            advantages.insert(0, advantage)\n",
        "\n",
        "            # 3. Update \"Next\" variables for the next iteration (which is i-1)\n",
        "            next_value = self.values[i]\n",
        "            next_advantage = advantage\n",
        "\n",
        "        return advantages\n"
      ],
      "metadata": {
        "id": "0jYlDAVKia7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "env = NashBrokerTrader()\n",
        "rolloutbuffer = RolloutBuffer()\n",
        "actor_test = PPOAgent_actor(4, 1)\n",
        "critic_test = PPOAgent_critic(4)\n",
        "\n",
        "state = env.reset()\n",
        "state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0) # Add a batch dimension\n",
        "\n",
        "'''The output is a random sample from a normal distribution'''\n",
        "\n",
        "#print(actor_test.get_dist(state_tensor)) #The probability distibution\n",
        "#print(actor_test.get_dist(state_tensor).sample()) #realisation of a probability distribution\n",
        "\n",
        "'''The output is a single value'''\n",
        "\n",
        "print(critic_test(state_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfeZeJ-Lsady",
        "outputId": "544c54d1-eaf3-466d-9cfd-aea4bfbc303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0913]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = NashBrokerTrader()\n",
        "rolloutbuffer = RolloutBuffer()\n",
        "actor = PPOAgent_actor(4, 1)\n",
        "critic = PPOAgent_critic(4)\n",
        "\n",
        "#Hyperparameters\n",
        "lr = 3e-4; gamma = 0.99; epochs = 10 #10 updates per batch\n",
        "lamda = 0.95; epsilon = 0.2; batch_size = 100\n",
        "\n",
        "actor_optimizer = torch.optim.Adam(actor.parameters(), lr)\n",
        "critic_optimizer = torch.optim.Adam(critic.parameters(), lr)\n",
        "\n",
        "num_updates = 100\n",
        "\n",
        "# --- Layer 1: Setup (Outside loops) ---\n",
        "state = env.reset()  # Reset ONCE at the very beginning\n",
        "\n",
        "# Loop for many updates (e.g., 1000 times)\n",
        "for update in range(num_updates):\n",
        "\n",
        "    # --- Layer 2: Data Collection (The Rollout) ---\n",
        "    for step in range(batch_size):\n",
        "\n",
        "        # === LAYER 3: YOUR CODE GOES HERE ===\n",
        "        # 0. Optimization: Turn off gradients\n",
        "        with torch.no_grad():\n",
        "            # 1. Turn state to tensor\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            # 2. Get action from Actor\n",
        "            distribution = actor.get_dist(state_tensor)\n",
        "            action = distribution.sample()\n",
        "            # 2.1. We sum() just in case action_dim > 1 (not strictly needed here but good practice)\n",
        "            log_prob = distribution.log_prob(action).sum()\n",
        "            # 3. Get value from Critic\n",
        "            value = critic(state_tensor)\n",
        "        # 4. Step the environment\n",
        "        # action is a tensor like [[-2.5]], we need the float -2.5\n",
        "        next_state, reward, done, _, _ = env.step(action.item())\n",
        "        # ====================================\n",
        "\n",
        "        # --- Layer 4: Storage & Cleanup (Crucial!) ---\n",
        "        # You must store the data before moving to the next step!\n",
        "        rolloutbuffer.add(state, action, log_prob, reward, done, value)\n",
        "\n",
        "        # Update state for the next loop iteration\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "\n",
        "    # --- Layer 5: The PPO Update (After the batch is full) ---\n",
        "    # 1. Bootstrap: Get value of the very last state (to handle the \"future\" for the last step)\n",
        "    with torch.no_grad():\n",
        "        next_value = critic(torch.tensor(state, dtype=torch.float32))\n",
        "    # 2. Calculate Advantages\n",
        "    # (Use the method you wrote in RolloutBuffer)\n",
        "    advantages = rolloutbuffer.compute_gae(next_value, gamma, lamda)\n",
        "    # 3. Prepare Data\n",
        "    # Convert python lists to one big tensor for training\n",
        "    # (flattening isn't strictly necessary if batch_size is small, but good practice)\n",
        "    tensor_states = torch.tensor(rolloutbuffer.states, dtype=torch.float32)\n",
        "    tensor_actions = torch.tensor(rolloutbuffer.actions, dtype=torch.float32)\n",
        "    tensor_old_log_probs = torch.tensor(rolloutbuffer.log_probs, dtype=torch.float32)\n",
        "    # TIP: Normalize Advantages (Crucial for stability!)\n",
        "    tensor_advantages = (tensor_advantages - tensor_advantages.mean()) / (tensor_advantages.std() + 1e-8)\n",
        "    # Calculate \"Returns\" (The target for the Critic)\n",
        "    # Return = Advantage + Value (Simple algebra from A = R - V)\n",
        "    tensor_returns = tensor_advantages + torch.tensor(rolloutbuffer.values, dtype=torch.float32)\n",
        "\n",
        "    # 4. PPO Optimization Epochs\n",
        "    # We update the same batch of data multiple times (epochs)\n",
        "    for _ in range(epochs):\n",
        "\n",
        "        # A. Re-evaluate the data with the CURRENT network\n",
        "        # (We need to see how the policy has changed since collection)\n",
        "        dist = actor.get_dist(tensor_states)\n",
        "        new_log_probs = dist.log_prob(tensor_actions).sum(axis=-1)\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        new_values = critic(tensor_states).squeeze()\n",
        "\n",
        "        # B. Calculate the Ratio r_t(theta)\n",
        "        # Hint: ratio = exp(new_log - old_log)\n",
        "        ratio = torch.exp(new_log_probs - tensor_old_log_probs)\n",
        "\n",
        "        # C. Calculate Surrogate Loss (L_CLIP)\n",
        "        # Hint: The min() logic we discussed!\n",
        "        # surrogate1 = ratio * advantage\n",
        "        # surrogate2 = clamp(ratio, 1-eps, 1+eps) * advantage\n",
        "        # actor_loss = -min(...)\n",
        "        # (Note: We use negative because optimizers minimize, but we want to maximize reward)\n",
        "        surr1 = ratio * tensor_advantages\n",
        "        surr2 = torch.clamp(ratio, 1-epsilon, 1+epsilon) * tensor_advantages\n",
        "\n",
        "        # FIX: Take the MEAN to get a single scalar loss\n",
        "        actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "        # D. Calculate Critic Loss (L_VF)\n",
        "        # MSE between new_values and b_returns\n",
        "        critic_loss = F.mse_loss(new_values, tensor_returns)\n",
        "\n",
        "        # E. Total Loss & Backprop\n",
        "        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
        "\n",
        "        actor_optimizer.zero_grad()\n",
        "        critic_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    # 5. Clean up\n",
        "    rolloutbuffer.clear()\n"
      ],
      "metadata": {
        "id": "KgfE6JkKu1rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''env = NashBrokerTrader()\n",
        "rolloutbuffer = RolloutBuffer()\n",
        "actor = PPOAgent_actor(4, 1)\n",
        "critic = PPOAgent_critic(4)\n",
        "\n",
        "#Hyperparameters\n",
        "lr = 3e-4; gamma = 0.99; epochs = 10 #10 updates per batch\n",
        "lamda = 0.95; epsilon = 0.2; batch_size = 100\n",
        "\n",
        "actor_optimizer = torch.optim.Adam(actor.parameters(), lr)\n",
        "critic_optimizer = torch.optim.Adam(critic.parameters(), lr)\n",
        "\n",
        "num_updates = 100\n",
        "\n",
        "# --- Layer 1: Setup (Outside loops) ---\n",
        "state = env.reset()  # Reset ONCE at the very beginning\n",
        "\n",
        "# Loop for many updates (e.g., 1000 times)\n",
        "for update in range(num_updates):\n",
        "\n",
        "    # --- Layer 2: Data Collection (The Rollout) ---\n",
        "    for step in range(batch_size):\n",
        "\n",
        "        # === LAYER 3: YOUR CODE GOES HERE ===\n",
        "        # 0. Optimization: Turn off gradients\n",
        "        with torch.no_grad():\n",
        "            # 1. Turn state to tensor\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            # 2. Get action from Actor\n",
        "            distribution = actor.get_dist(state_tensor)\n",
        "            action = distribution.sample()\n",
        "            # 2.1. We sum() just in case action_dim > 1 (not strictly needed here but good practice)\n",
        "            log_prob = distribution.log_prob(action).sum()\n",
        "            # 3. Get value from Critic\n",
        "            value = critic(state_tensor)\n",
        "        # 4. Step the environment\n",
        "        # action is a tensor like [[-2.5]], we need the float -2.5\n",
        "        next_state, reward, done, _, _ = env.step(action.item())\n",
        "        # ====================================\n",
        "\n",
        "        # --- Layer 4: Storage & Cleanup (Crucial!) ---\n",
        "        # You must store the data before moving to the next step!\n",
        "        rolloutbuffer.add(state, action, log_prob, reward, done, value)\n",
        "\n",
        "        # Update state for the next loop iteration\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "\n",
        "    # --- Layer 5: The PPO Update (After the batch is full) ---\n",
        "    # 1. Bootstrap: Get value of the very last state (to handle the \"future\" for the last step)\n",
        "    with torch.no_grad():\n",
        "        next_value = critic(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "    # 2. Calculate Advantages\n",
        "    # (Use the method you wrote in RolloutBuffer)\n",
        "    advantages = rolloutbuffer.compute_gae(next_value, gamma, lamda)\n",
        "\n",
        "    # 3. Prepare Data\n",
        "    # Convert python lists to one big tensor for training\n",
        "    tensor_states = torch.tensor(np.array(rolloutbuffer.states), dtype=torch.float32)\n",
        "    tensor_actions = torch.tensor(np.array(rolloutbuffer.actions), dtype=torch.float32)\n",
        "    tensor_old_log_probs = torch.tensor(np.array(rolloutbuffer.log_probs), dtype=torch.float32)\n",
        "    # Ensure advantages is a tensor and then flatten if necessary\n",
        "    tensor_advantages = torch.cat(advantages).squeeze(-1) if advantages and len(advantages[0].shape) > 0 else torch.tensor(advantages, dtype=torch.float32)\n",
        "    # Calculate \"Returns\" (The target for the Critic)\n",
        "    tensor_returns = tensor_advantages + torch.tensor(np.array(rolloutbuffer.values), dtype=torch.float32).squeeze(-1)\n",
        "\n",
        "    # 4. PPO Optimization Epochs\n",
        "    # We update the same batch of data multiple times (epochs)\n",
        "    for _ in range(epochs):\n",
        "\n",
        "        # A. Re-evaluate the data with the CURRENT network\n",
        "        # (We need to see how the policy has changed since collection)\n",
        "        dist = actor.get_dist(tensor_states)\n",
        "        new_log_probs = dist.log_prob(tensor_actions).sum(axis=-1)\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        new_values = critic(tensor_states).squeeze()\n",
        "\n",
        "        # B. Calculate the Ratio r_t(theta)\n",
        "        ratio = torch.exp(new_log_probs - tensor_old_log_probs)\n",
        "\n",
        "        # C. Calculate Surrogate Loss (L_CLIP)\n",
        "        clip_ratio = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
        "        actor_loss = -torch.min(ratio * tensor_advantages, clip_ratio * tensor_advantages).mean() # .mean() added here\n",
        "\n",
        "        # D. Calculate Critic Loss (L_VF)\n",
        "        critic_loss = F.mse_loss(new_values, tensor_returns)\n",
        "\n",
        "        # E. Total Loss & Backprop\n",
        "        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
        "\n",
        "        actor_optimizer.zero_grad()\n",
        "        critic_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    # 5. Clean up\n",
        "    rolloutbuffer.clear()'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBoCneHlwRx-",
        "outputId": "ea18b9be-370e-405c-f816-2f5bb693091c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-698081082.py:89: UserWarning: Using a target size (torch.Size([100, 100])) that is different to the input size (torch.Size([100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  critic_loss = F.mse_loss(new_values, tensor_returns)\n"
          ]
        }
      ]
    }
  ]
}