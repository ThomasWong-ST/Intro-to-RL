{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo5CjbbqXkEKBnpjC1400j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasWong-ST/Intro-to-RL/blob/main/RL_and_Monte_Carlo_Sim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "LrKMvGvNI4q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "Estimate the integral of $e^{2}$ from 0 to N using the Monte Carlo method. Where N can be any number."
      ],
      "metadata": {
        "id": "5W14G6Mio1r9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhl5-e6QIeC_",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2c1dfc-de5e-488a-c8ac-5cb12c555d19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(2.6783640877375077)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#MC simulation for x^2\n",
        "X_sample = []\n",
        "for i in range(10000):\n",
        "  X_sample.append(np.random.uniform(0,2))\n",
        "  i = i+1\n",
        "#print(X_sample)\n",
        "\n",
        "def x_squared(sampled_data):\n",
        "  for i in range(len(sampled_data)):\n",
        "    sampled_data[i] = sampled_data[i]**2\n",
        "    i = i+1\n",
        "  return sampled_data\n",
        "\n",
        "(2-0)/len(X_sample) * np.array(x_squared(X_sample)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84db42c0"
      },
      "source": [
        "# Task\n",
        "Estimate the integral of $e^{-x^2}$ from -10 to 10 using the Monte Carlo method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e27119c",
        "outputId": "e3e66b3c-9c8c-4f43-ccec-a8d457697fb3"
      },
      "source": [
        "import numpy as np\n",
        "from math import sqrt, pi\n",
        "\n",
        "N = 1000000\n",
        "X = np.random.normal(0.0, 1, size=N)\n",
        "\n",
        "f_vals = np.exp(-X**2)                         # f(x) = e^{-x^2}\n",
        "q_vals = (1/np.sqrt(2*np.pi)) * np.exp(-X**2/2) # standard normal pdf\n",
        "weights = f_vals / q_vals                      # = sqrt(2π) * exp(-X^2/2)\n",
        "\n",
        "I_hat = weights.mean()\n",
        "print(I_hat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7725805232251632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "Grid World MC Predictions."
      ],
      "metadata": {
        "id": "K988q5j0ah2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
        "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, n=4, step_cost=-0.01, terminal_reward=1.0, seed=0):\n",
        "        self.n = n\n",
        "        self.step_cost = step_cost\n",
        "        self.terminal = (n-1, n-1)\n",
        "        self.terminal_reward = terminal_reward\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.state = None  # (row, col)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset to a random non-terminal cell; return state index.\"\"\"\n",
        "        while True:\n",
        "            r = int(self.rng.integers(self.n))\n",
        "            c = int(self.rng.integers(self.n))\n",
        "            if (r, c) != self.terminal:\n",
        "                self.state = (r, c)\n",
        "                return self._to_index(self.state)#maps each element of the grid to an integer, (0,0)->0, (0,1)->1, (0,2)->2,...,(2,3)->11,...,(3,3)->16\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Apply action, return: next_state_idx, reward, done, info_dict\n",
        "        Rules:\n",
        "        - Move in grid if possible, else stay.\n",
        "        - Reaching terminal yields terminal_reward and done=True.\n",
        "        - Otherwise reward = step_cost and done=False.\n",
        "        \"\"\"\n",
        "        r, c = self.state\n",
        "        nr, nc = r, c  # TODO: compute next (nr, nc) from action with boundary checks\n",
        "        if action == 0:       # UP\n",
        "          nr = r - 1\n",
        "        elif action == 1:     # DOWN\n",
        "            nr = r + 1\n",
        "        elif action == 2:     # LEFT\n",
        "            nc = c - 1\n",
        "        elif action == 3:     # RIGHT\n",
        "            nc = c + 1\n",
        "        if not (0 <= nr < self.n and 0 <= nc < self.n):\n",
        "            nr, nc = r, c\n",
        "\n",
        "        # TODO: set self.state = (nr, nc)\n",
        "        self.state = (nr, nc)\n",
        "        # TODO: compute reward and done based on whether (nr, nc) == terminal\n",
        "        if (nr, nc) == self.terminal:\n",
        "            reward = self.terminal_reward\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "            reward = self.step_cost\n",
        "        # return self._to_index(self.state), reward, done, {}\n",
        "        return self._to_index(self.state), reward, done, {} # a 4-tuple → (int, float, bool, dict)\n",
        "\n",
        "    # --- helpers ---\n",
        "    def _to_index(self, rc):#rc = self.state\n",
        "        r, c = rc\n",
        "        return r * self.n + c\n",
        "\n",
        "    def _from_index(self, idx):\n",
        "        return divmod(idx, self.n)\n",
        "\n",
        "    @property\n",
        "    def nS(self):  # number of states\n",
        "        return self.n * self.n\n",
        "\n",
        "    @property\n",
        "    def nA(self):  # number of actions\n",
        "        return len(ACTIONS)"
      ],
      "metadata": {
        "id": "b6NnRtYgaeBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode(env, policy, max_steps=100):\n",
        "    states, actions, rewards = [], [], []\n",
        "    state = env.reset()\n",
        "    for _ in range(max_steps):\n",
        "        action = policy(state)\n",
        "        #print(generate_episode.__closure__)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "8XB6jeDUdA28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld()\n",
        "episode = generate_episode(env, policy=lambda s: np.random.choice(env.nA))\n",
        "print(episode)\n",
        "#print(episode[0])\n",
        "#print(episode[1])\n",
        "#print(episode[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMZl1fTRg4OL",
        "outputId": "6c177f66-0edd-4772-e47f-d9690977ecd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([14, 10, 14, 10, 9, 13, 13, 13, 13, 14, 10, 9, 8, 9, 13, 13, 9, 10, 14, 14, 10, 14, 13, 12, 12, 12, 12, 12, 8, 8, 4, 5, 6, 5, 9, 8, 12, 12, 8, 9, 13, 14, 10, 6, 7, 3, 7, 11, 10, 11, 11, 7, 7, 7, 11], [0, 1, 0, 2, 1, 1, 1, 1, 3, 0, 2, 2, 3, 1, 1, 0, 3, 1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 0, 2, 0, 3, 3, 2, 1, 2, 1, 1, 0, 3, 1, 3, 0, 0, 3, 0, 1, 1, 2, 3, 3, 0, 3, 3, 1, 1], [-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, 1.0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns = {s: [] for s in range(env.nS)}   # if you want the textbook version"
      ],
      "metadata": {
        "id": "h_Xz760Gg4gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_prediction(env, policy, num_episodes=1000, gamma=1.0):\n",
        "    V = np.zeros(env.nS)\n",
        "    N = np.zeros(env.nS)   # counts visits\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        states, actions, rewards = generate_episode(env, policy)\n",
        "\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        # backward loop\n",
        "        for t in reversed(range(len(states))):\n",
        "            G = gamma * G + rewards[t]\n",
        "            s = states[t]\n",
        "            if s not in visited:  # first-visit\n",
        "                visited.add(s)\n",
        "                N[s] += 1\n",
        "                V[s] += (G - V[s]) / N[s]\n",
        "    return V"
      ],
      "metadata": {
        "id": "z7ogWx-PJdRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_prediction(env, policy=lambda s: np.random.choice(env.nA), num_episodes=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "F2s1QHVoXFwr",
        "outputId": "761c0316-8728-4149-a95e-26a3473b08c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'env' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1500983645.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmc_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- lookahead that does NOT mutate env ----------\n",
        "def lookahead(env, state, action):\n",
        "    r, c = env._from_index(state)\n",
        "    nr, nc = r, c\n",
        "    if action == 0: nr -= 1\n",
        "    elif action == 1: nr += 1\n",
        "    elif action == 2: nc -= 1\n",
        "    elif action == 3: nc += 1\n",
        "    if not (0 <= nr < env.n and 0 <= nc < env.n):\n",
        "        nr, nc = r, c\n",
        "    next_state = env._to_index((nr, nc))\n",
        "    reward = env.terminal_reward if (nr, nc) == env.terminal else env.step_cost\n",
        "    done = (nr, nc) == env.terminal\n",
        "    return next_state, reward, done\n",
        "\n",
        "# ---------- greedy and ε-greedy policies from V ----------\n",
        "def make_greedy_policy(V, env, gamma=1.0):\n",
        "    def policy(state):\n",
        "        #print(policy.__closure__)\n",
        "        qs = []\n",
        "        for a in range(env.nA):\n",
        "            ns, r, done = lookahead(env, state, a)\n",
        "            q = r if done else (r + gamma * V[ns])  # one-step lookahead using V\n",
        "            qs.append(q)\n",
        "        return int(np.argmax(qs))\n",
        "    return policy\n",
        "\n",
        "def make_eps_greedy_policy(V, env, eps=0.1, gamma=1.0, seed=0):\n",
        "    greedy = make_greedy_policy(V, env, gamma)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    def policy(state):\n",
        "        #print(policy.__closure__)\n",
        "        if rng.random() < eps:\n",
        "            return int(rng.integers(env.nA))\n",
        "        return greedy(state)\n",
        "    return policy\n",
        "\n",
        "# ---------- simple runner to evaluate a policy ----------\n",
        "def run_policy(env, policy, n_episodes=100, max_steps=200, gamma=1.0):\n",
        "    returns = []\n",
        "    lengths = []\n",
        "    for _ in range(n_episodes):\n",
        "        states, actions, rewards = generate_episode(env, policy, max_steps=max_steps)\n",
        "        # discounted return from t=0 (MC return)\n",
        "        G = 0.0\n",
        "        for r in reversed(rewards):\n",
        "            G = gamma * G + r\n",
        "        returns.append(G)\n",
        "        lengths.append(len(actions))\n",
        "    return {\n",
        "        \"avg_return\": float(np.mean(returns)),\n",
        "        \"std_return\": float(np.std(returns)),\n",
        "        \"avg_length\": float(np.mean(lengths)),\n",
        "        \"episodes\": n_episodes,\n",
        "    }"
      ],
      "metadata": {
        "id": "CVa7SPJCYN5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- example usage ----------\n",
        "env = GridWorld(n=4, step_cost=-0.01, terminal_reward=1.0, seed=42)\n",
        "\n",
        "# Your learned V:\n",
        "V_interataion_1 = mc_prediction(env, policy=lambda s: np.random.choice(env.nA), num_episodes=10000)\n",
        "\n",
        "greedy_pol_interataion_1 = make_greedy_policy(V_interataion_1, env, gamma=1.0)\n",
        "epsgreedy_pol_interataion_1 = make_eps_greedy_policy(V_interataion_1, env, eps=0.1, gamma=1.0, seed=42)\n",
        "\n",
        "# Try both:\n",
        "stats_greedy = run_policy(env, greedy_pol_interataion_1, n_episodes=200)\n",
        "stats_eps    = run_policy(env, epsgreedy_pol_interataion_1, n_episodes=200)\n",
        "\n",
        "print(\"Greedy policy:\", stats_greedy)\n",
        "print(\"ε-greedy (ε=0.1):\", stats_eps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtL_ipZJMWM7",
        "outputId": "f448abef-d5f3-4adc-d3c7-5411deb0c418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy policy: {'avg_return': 0.97775, 'std_return': 0.014471955638406317, 'avg_length': 3.225, 'episodes': 200}\n",
            "ε-greedy (ε=0.1): {'avg_return': 0.9753499999999999, 'std_return': 0.017772942918942842, 'avg_length': 3.465, 'episodes': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(greedy_pol.__closure__, epsgreedy_pol.__closure__, lookahead.__closure__, run_policy.__closure__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ-YjcPuAZRf",
        "outputId": "c2c17511-f01f-4622-ebfc-16cfc4c6ad6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<cell at 0x7ce7950d6a70: numpy.ndarray object at 0x7ce794a21f50>, <cell at 0x7ce7950d7460: GridWorld object at 0x7ce7950d5f10>, <cell at 0x7ce7950d4040: float object at 0x7ce7950e5010>) (<cell at 0x7ce7950735e0: GridWorld object at 0x7ce7950d5f10>, <cell at 0x7ce7963449a0: float object at 0x7ce796366b90>, <cell at 0x7ce796345510: function object at 0x7ce796362e80>, <cell at 0x7ce7950d5a50: numpy.random._generator.Generator object at 0x7ce796337680>) None None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use the new greedy policy and Value function to re-generate the episdoes and value functions\n",
        "print(generate_episode(env, greedy_pol_interataion_1, max_steps=200))\n",
        "print(mc_prediction(env, policy= greedy_pol_interataion_1, num_episodes=1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YEx7mVBQQFn",
        "outputId": "1a8b0301-95f8-4f83-a8a6-da1df603335d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([14], [3], [1.0])\n",
            "[0.95 0.96 0.97 0.98 0.96 0.97 0.98 0.99 0.97 0.98 0.99 1.   0.98 0.99\n",
            " 1.   0.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can do policy improvement, i.e. improving our greedy policy by using the new V_interataion_2 and applying them onto our greedy_policy\n",
        "V_interataion_2 = mc_prediction(env, policy=epsgreedy_pol_interataion_1, num_episodes=1000)\n",
        "greedy_pol_interataion_2 = make_greedy_policy(V_interataion_2, env, gamma=1.0)"
      ],
      "metadata": {
        "id": "7-eNsyF_XUcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_episode(env, greedy_pol_interataion_2, max_steps=200))\n",
        "print(mc_prediction(env, policy= greedy_pol_interataion_2, num_episodes=1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWXJ5hHKXcVA",
        "outputId": "ef77d441-beb2-4d2a-eb46-6e2adf188f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([9, 10, 14], [3, 1, 3], [-0.01, -0.01, 1.0])\n",
            "[0.95 0.96 0.97 0.98 0.96 0.97 0.98 0.99 0.97 0.98 0.99 1.   0.98 0.99\n",
            " 1.   0.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Function with functions\n",
        "Understand how the variable state in the function lookahead is defined, and how does the def and local scope work.\n",
        "\n",
        "1. Local Scope: Variables defined inside a function, like y in level_two, have a local scope. This means they only exist and are accessible within that specific function. Once the function finishes executing, the local scope is destroyed, and the variables defined within it are no longer available.\n",
        "\n",
        "2. def and Scope: The def keyword is used to define functions, and defining a function creates a new local scope for the variables defined within that function's body. This is a key mechanism for organizing code and preventing unintended interference between different parts of your program.\n",
        "\n",
        "3. The def keyword establishes a boundary (the local scope). When you refer to a variable name inside the function, Python first looks for it within that function's local scope. If it finds it, it uses that local variable. If it doesn't find it locally, it then looks in progressively wider scopes (like the global scope) following the LEGB rule (Local, Enclosing function locals, Global, Built-in). When you try to access y outside the function, Python only looks in the global scope and doesn't find y defined there, hence the NameError.\n",
        "\n",
        "4. Function definitions created with def are the most common way to create a new local scope in Python. However, other constructs like classes also create their own namespaces (which are similar to scopes for attributes and methods), and list comprehensions, dictionary comprehensions, set comprehensions, and generator expressions also have their own local scopes. But for defining reusable blocks of code with their own isolated variables, def is the primary tool."
      ],
      "metadata": {
        "id": "6QQ-VX4v678v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x = \"global x\"\n",
        "def level_two(v):\n",
        "  print(v)\n",
        "  if v:\n",
        "    y = \"local y\"\n",
        "  return y"
      ],
      "metadata": {
        "id": "9WuXmwwJsNlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "level_two(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "M4I2C4hwsdtY",
        "outputId": "91db9b8c-7b1f-42cc-f3cb-17f2a21159ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'local y'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "JTVmcMVDsisN",
        "outputId": "dceffcdc-1347-4b3f-8a51-074d3f726ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1056546137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def x1():\n",
        "  x = \"x1\"\n",
        "  return x\n",
        "def x2():\n",
        "  x = \"x1\"\n",
        "  return x"
      ],
      "metadata": {
        "id": "rstZllRd3BpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1(),x2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9QkEJw84urG",
        "outputId": "0856e51e-fbd9-4d21-d6b2-1a86e38571ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('x1', 'x2')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = [0,1,2,3]\n",
        "x = []\n",
        "def test_loop1():\n",
        "  for i in range(len(test_list)):\n",
        "    x.append(test_list[i]+1) # Access the element at index i\n",
        "  return x\n",
        "def test_loop2():\n",
        "  for i in range(len(test_list)):\n",
        "    x.append(test_list[i]+2) # Access the element at index i\n",
        "  return x"
      ],
      "metadata": {
        "id": "6ws1BEdK4xuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "test_loop1(),test_loop2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bz5Edty5657",
        "outputId": "8fda0fb2-03b4-4fc2-b77b-78430c7db926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2, 3, 4, 2, 3, 4, 5], [1, 2, 3, 4, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}