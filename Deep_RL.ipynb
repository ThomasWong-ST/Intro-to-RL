{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl/W3i+NpuLSzAOO9/V0XK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasWong-ST/Intro-to-RL/blob/main/Deep_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "9FiKwH6NCeCq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DQN Implementation Example"
      ],
      "metadata": {
        "id": "oBx493HDxIX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define layers: for example, 1 input -> 10 hidden -> 10 hidden -> 1 output\n",
        "        self.hidden1 = nn.Linear(1, 10)   # linear layer (1 -> 10)\n",
        "        self.hidden2 = nn.Linear(10, 10)  # linear layer (10 -> 10)\n",
        "        self.output = nn.Linear(10, 1)    # linear layer (10 -> 1)\n",
        "        self.activation = nn.ReLU()       # ReLU activation for hidden layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass: apply linear layers and activation\n",
        "        x = self.activation(self.hidden1(x))\n",
        "        x = self.activation(self.hidden2(x))\n",
        "        x = self.output(x)  # output layer (no activation for regression)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = Net()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wRHE0TnCcab",
        "outputId": "65919ebb-6eb4-4481-fc75-6cd7046c22b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (hidden1): Linear(in_features=1, out_features=10, bias=True)\n",
            "  (hidden2): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (output): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (activation): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()                   # Mean Squared Error loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "F3FllOBvCsiO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example function and data: let's approximate f(x) = sin(x)^2 on [0, 2]\n",
        "# Generate training data (e.g., 20 random points in [0,2])\n",
        "x_train = 2 * torch.rand(20, 1)            # shape (20,1) inputs in [0,2]\n",
        "y_train = torch.sin(x_train) ** 2          # shape (20,1) outputs f(x) = sin^2(x)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()                 # 1. zero out gradients from previous step\n",
        "    y_pred = model(x_train)               # 2. forward pass: compute predictions\n",
        "    loss = criterion(y_pred, y_train)     # 3. compute loss (MSE between y_pred and y_train)\n",
        "    loss.backward()                       # 4. backward pass: compute gradients dL/dw for each param\n",
        "    optimizer.step()                      # 5. update weights: w <- w - lr * grad\n",
        "\n",
        "    if (epoch+1) % 100 == 0:  # print loss every 100 epochs\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss = {loss.item():.6f}\")"
      ],
      "metadata": {
        "id": "HOpsXLm7C2W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A Non-Linear Function Approximation\n",
        "Environment Description: Imagine a one-dimensional track from -1.0 to +1.0. The agent starts at position 0.0. It has two actions: move left (decrease position) or move right (increase position) by a fixed step. If the agent reaches the left end (-1.0) or the right end (+1.0), it receives a reward of +1 and the episode ends. If the agent fails to reach either end within a certain number of steps, the episode ends with 0 reward. This environment is deterministic and fully observable (state = current position)."
      ],
      "metadata": {
        "id": "HMqpDnYRxSB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleGoalEnv:\n",
        "    def __init__(self):\n",
        "        # Define the 1D state space boundaries\n",
        "        self.min_position = -1.0\n",
        "        self.max_position =  1.0\n",
        "        self.step_size    =  0.1    # movement increment for each action\n",
        "        self.max_steps    = 50     # episode terminates if this many steps elapse without reaching a goal\n",
        "        self.state        = None\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment to the starting state.\"\"\"\n",
        "        self.state = 0.0               # start at the center\n",
        "        self.current_step = 0\n",
        "        # Return state as a NumPy array (for compatibility with PyTorch later)\n",
        "        return np.array([self.state], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executes one action in the environment.\n",
        "        Action: 0 = move left, 1 = move right.\n",
        "        Returns: next_state, reward, done, info\n",
        "        \"\"\"\n",
        "        # Validate action\n",
        "        if action not in [0, 1]:\n",
        "            raise ValueError(\"Invalid action. Must be 0 or 1.\")\n",
        "        # Determine new position after the action\n",
        "        if action == 1:  # move right\n",
        "            new_state = self.state + self.step_size\n",
        "        else:            # move left\n",
        "            new_state = self.state - self.step_size\n",
        "        # Increase step count\n",
        "        self.current_step += 1\n",
        "        # Avoid floating-point accumulation errors by rounding\n",
        "        new_state = float(np.round(new_state, 5))\n",
        "\n",
        "        # Initialize reward and done flag\n",
        "        reward = 0.0\n",
        "        done   = False\n",
        "        # Check if the new state crosses or reaches the goal boundaries\n",
        "        if new_state >= self.max_position:\n",
        "            self.state = self.max_position\n",
        "            reward = 1.0\n",
        "            done   = True    # reached right end -> success\n",
        "        elif new_state <= self.min_position:\n",
        "            self.state = self.min_position\n",
        "            reward = 1.0\n",
        "            done   = True    # reached left end -> success\n",
        "        else:\n",
        "            # Not yet at a goal, just update state\n",
        "            self.state = new_state\n",
        "        # If max steps exceeded and no goal reached, end the episode (failure)\n",
        "        if self.current_step >= self.max_steps and not done:\n",
        "            done   = True\n",
        "            reward = 0.0    # no goal reached within time limit\n",
        "\n",
        "        # Return next state as a float32 array, reward, termination flag, and an empty info dict\n",
        "        return np.array([self.state], dtype=np.float32), reward, done, {}"
      ],
      "metadata": {
        "id": "limvWVmHxXec"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DQN Implementation Test"
      ],
      "metadata": {
        "id": "9Q79N2iaVVuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NNet(nn.Module):\n",
        "    def __init__(self, input_dim = 1, output_dim = 2):\n",
        "        super(NNet, self).__init__()\n",
        "        # Define layers: for example, 1 input -> 10 hidden -> 10 hidden -> 1 output\n",
        "        self.hidden1 = nn.Linear(input_dim, 10)   # linear layer (1 -> 10)\n",
        "        self.hidden2 = nn.Linear(10, 10)  # linear layer (10 -> 10)\n",
        "        self.output = nn.Linear(10, output_dim)    # linear layer (10 -> 1)\n",
        "        self.activation = nn.ReLU()       # ReLU activation for hidden layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass: apply linear layers and activation\n",
        "        x = self.activation(self.hidden1(x))\n",
        "        x = self.activation(self.hidden2(x))\n",
        "        x = self.output(x)  # output layer (no activation for regression)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = NNet()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUR6KCBaU0fz",
        "outputId": "22193328-a3f5-4767-ee35-06e5976871e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNet(\n",
            "  (hidden1): Linear(in_features=1, out_features=10, bias=True)\n",
            "  (hidden2): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (output): Linear(in_features=10, out_features=2, bias=True)\n",
            "  (activation): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)  # automatically drops oldest when full\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save a transition.\"\"\"\n",
        "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Randomly sample a batch of transitions.\"\"\"\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "GQqOM55sUyAj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(q_net, state, epsilon):\n",
        "    \"\"\"\n",
        "    state: np.array or torch.tensor shape [input_dim] or [1, input_dim]\n",
        "    returns: action index (int)\n",
        "    \"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        # explore\n",
        "        return random.randrange(2)   # 2 actions: 0 or 1\n",
        "\n",
        "    # exploit\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "\n",
        "    if state.dim() == 1:\n",
        "        state = state.unsqueeze(0)   # [1, input_dim]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        q_values = q_net(state)      # [1, 2]\n",
        "        action = q_values.argmax(dim=1).item()\n",
        "    return action"
      ],
      "metadata": {
        "id": "AMU5ze8_LFud"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dqn_loss(batch, q_net, target_q_net, gamma, criterion):\n",
        "    # batch is a list of Transition\n",
        "    batch = Transition(*zip(*batch))\n",
        "    # batch.state is a tuple of states, etc.\n",
        "\n",
        "    # Fix: Convert batch.state/next_state to numpy array first to avoid extra dimension\n",
        "    # This ensures states and next_states are [B, 1], which leads to q_values being [B, 2]\n",
        "    states      = torch.tensor(np.array(batch.state),      dtype=torch.float32)  # Changed: Removed .unsqueeze(1)\n",
        "    actions     = torch.tensor(batch.action,     dtype=torch.long).unsqueeze(1)     # [B, 1]\n",
        "    rewards     = torch.tensor(batch.reward,     dtype=torch.float32).unsqueeze(1)  # [B, 1]\n",
        "    next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32)  # Changed: Removed .unsqueeze(1)\n",
        "    dones       = torch.tensor(batch.done,       dtype=torch.float32).unsqueeze(1)  # [B, 1]\n",
        "\n",
        "    # 1) Q_pred = Q_online(s,a; θ)\n",
        "    q_values = q_net(states)                     # [B, 2] now (2 dimensions)\n",
        "    q_pred = q_values.gather(1, actions)         # pick Q(s,a) → [B, 1] - now works as actions is also 2 dimensions\n",
        "\n",
        "    # 2) y = r + γ * (1 - done) * max_a' Q_target(s',a'; θ⁻)\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_q_net(next_states)           # [B, 2] now (2 dimensions)\n",
        "        max_next_q = next_q_values.max(dim=1, keepdim=True)[0]   # [B, 1]\n",
        "        targets = rewards + gamma * (1.0 - dones) * max_next_q   # [B, 1]\n",
        "\n",
        "    loss = criterion(q_pred, targets)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "zjuJHHPYkQBH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer = ReplayBuffer(capacity=10_000)\n",
        "num_episodes = 500\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "target_update_freq = 10   # episodes\n",
        "\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay = 300       # higher = slower decay\n",
        "\n",
        "step_count = 0\n",
        "env = DoubleGoalEnv()\n",
        "\n",
        "q_net = NNet(input_dim=1, output_dim=2)\n",
        "target_q_net = NNet(input_dim=1, output_dim=2)\n",
        "# copy weights from q_net → target_q_net initially\n",
        "target_q_net.load_state_dict(q_net.state_dict())\n",
        "target_q_net.eval()  # we don't train this with gradients\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()   # assume scalar like 0.0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # decay epsilon over time\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
        "                  torch.exp(torch.tensor(-step_count / epsilon_decay)).item()\n",
        "\n",
        "        action = select_action(q_net, state, epsilon)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # store transition\n",
        "        buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        step_count += 1\n",
        "\n",
        "        # learn if we have enough samples\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = buffer.sample(batch_size)\n",
        "            loss = compute_dqn_loss(batch, q_net, target_q_net, gamma, criterion)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # periodically update target network\n",
        "    if (episode + 1) % target_update_freq == 0:\n",
        "        target_q_net.load_state_dict(q_net.state_dict())\n"
      ],
      "metadata": {
        "id": "gKxX6TuwkXo1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q-Network Implementation:\n",
        "Below, we define a neural network using PyTorch's nn.Module. It has a couple of fully-connected layers with non-linear activations (ReLU). The final layer outputs one Q-value per action. We keep the network small (given the simple state space) – this also ensures it trains quickly in Colab. The code comments explain the torch.nn components:"
      ],
      "metadata": {
        "id": "cH9h3SmUPOBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network for Q-value approximation\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Define feed-forward neural network layers\n",
        "        self.fc1 = nn.Linear(input_dim, 64)   # first hidden layer with 64 units\n",
        "        self.fc2 = nn.Linear(64, 64)          # second hidden layer with 64 units\n",
        "        self.fc3 = nn.Linear(64, output_dim)  # output layer (Q-values for each action)\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of shape [batch_size, input_dim]\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # No activation on output layer (we want raw Q-values)\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "mjqgRQ-KEAaW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment and Q-network\n",
        "env = DoubleGoalEnv()\n",
        "state_dim  = 1            # state is one-dimensional (position)\n",
        "action_dim = 2            # two actions: 0 or 1\n",
        "q_net = QNetwork(state_dim, action_dim)\n",
        "optimizer = torch.optim.Adam(q_net.parameters(), lr=0.001)\n",
        "\n",
        "# Replay memory to store past transitions\n",
        "memory = deque(maxlen=10000)\n",
        "\n",
        "# Hyperparameters\n",
        "episodes = 200           # number of episodes to train\n",
        "batch_size = 32\n",
        "gamma = 0.99             # discount factor for future rewards\n",
        "epsilon = 1.0            # initial exploration probability\n",
        "epsilon_min = 0.05       # minimum exploration probability\n",
        "epsilon_decay = 0.995    # decay rate per episode\n",
        "\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()                        # reset environment at start of episode\n",
        "    total_reward = 0.0\n",
        "    done = False\n",
        "    # Iterate for each step of the episode\n",
        "    while not done:\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randint(0, action_dim-1)  # explore: random action\n",
        "        else:\n",
        "            # exploit: choose best action according to Q-network\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # shape [1,1]\n",
        "            q_values = q_net(state_tensor)       # get Q-values for both actions\n",
        "            action = int(torch.argmax(q_values, dim=1).item())  # select action with max Q\n",
        "\n",
        "        # Apply action to the environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        # Store transition in replay buffer\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        # Update current state\n",
        "        state = next_state\n",
        "\n",
        "        # Train the Q-network with a batch sampled from memory (if enough samples exist)\n",
        "        if len(memory) >= batch_size:\n",
        "            # Sample a random minibatch of transitions\n",
        "            batch = random.sample(memory, batch_size)\n",
        "            # Separate the components of each transition\n",
        "            states  = np.vstack([transition[0] for transition in batch])   # shape (batch_size, state_dim)\n",
        "            actions = np.array([transition[1] for transition in batch])\n",
        "            rewards = np.array([transition[2] for transition in batch], dtype=np.float32)\n",
        "            next_states = np.vstack([transition[3] for transition in batch])\n",
        "            dones   = np.array([transition[4] for transition in batch], dtype=np.float32)\n",
        "\n",
        "            # Convert to tensors\n",
        "            state_tensor      = torch.tensor(states, dtype=torch.float32)        # shape [B, 1]\n",
        "            next_state_tensor = torch.tensor(next_states, dtype=torch.float32)   # shape [B, 1]\n",
        "            action_tensor     = torch.tensor(actions, dtype=torch.int64)         # shape [B]\n",
        "            reward_tensor     = torch.tensor(rewards, dtype=torch.float32)       # shape [B]\n",
        "            done_tensor       = torch.tensor(dones, dtype=torch.float32)         # shape [B]\n",
        "\n",
        "            # Compute current Q values for the actions taken: Q(s, a)\n",
        "            q_values = q_net(state_tensor)            # shape [B, action_dim]\n",
        "            # Gather the Q-values for the chosen actions\n",
        "            # Using unsqueeze to align indices: shape of chosen_q = [B]\n",
        "            chosen_q = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Compute target Q values: if done, target = reward; else target = reward + γ * max_a' Q_next\n",
        "            with torch.no_grad():\n",
        "                # Get max Q for next state\n",
        "                next_q_values = q_net(next_state_tensor)               # [B, action_dim]\n",
        "                max_next_q = torch.max(next_q_values, dim=1)[0]        # [B]\n",
        "                target = reward_tensor + gamma * max_next_q * (1 - done_tensor)\n",
        "                # (When done_tensor is 1, the term (1-done) will zero out the future reward.)\n",
        "\n",
        "            # Compute loss (Mean Squared Error between targets and current Q estimates)\n",
        "            loss = F.mse_loss(chosen_q, target)\n",
        "\n",
        "            # Backpropagation and optimization step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Decay exploration rate each episode\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    # Optionally, print training progress\n",
        "    if episode % 20 == 0:\n",
        "        print(f\"Episode {episode}: Total Reward = {total_reward:.1f}, Epsilon = {epsilon:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqLcE05jX7cu",
        "outputId": "34fc99f6-c133-4a48-c2a0-f6bf0b24095b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Total Reward = 1.0, Epsilon = 0.905\n",
            "Episode 40: Total Reward = 1.0, Epsilon = 0.818\n",
            "Episode 60: Total Reward = 1.0, Epsilon = 0.740\n",
            "Episode 80: Total Reward = 0.0, Epsilon = 0.670\n",
            "Episode 100: Total Reward = 1.0, Epsilon = 0.606\n",
            "Episode 120: Total Reward = 1.0, Epsilon = 0.548\n",
            "Episode 140: Total Reward = 1.0, Epsilon = 0.496\n",
            "Episode 160: Total Reward = 1.0, Epsilon = 0.448\n",
            "Episode 180: Total Reward = 1.0, Epsilon = 0.406\n",
            "Episode 200: Total Reward = 1.0, Epsilon = 0.367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode(env, Q, epsilon=0.0, max_steps=1000):\n",
        "    \"\"\"\n",
        "    generate_episode just reproduces the sets of actions that follows the largest Q-value\n",
        "    from the Q-table created by either SARSA or Q-learning. We can set epsilon = 0,\n",
        "    (unless we want to explore differnt Q-values) and return a list of\n",
        "    states, actions and rewards for the Q-table used.\n",
        "\n",
        "    Generate one episode using the learned Q-table.\n",
        "    If epsilon > 0, follow an epsilon-greedy policy (exploration).\n",
        "    If epsilon = 0, follow a greedy policy (pure exploitation).\n",
        "    \"\"\"\n",
        "    states, actions, rewards = [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        # --- ε-greedy action from Q ---\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(env.nA)\n",
        "        else:\n",
        "            q = Q[state]\n",
        "            best = np.flatnonzero(q == q.max())\n",
        "            action = int(np.random.choice(best))\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "Il0GX4apZXpj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_episode(env, q_values, epsilon=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbRlxRROZZjJ",
        "outputId": "51d89538-188d-412a-ae5e-df9338c7ca6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([0.], dtype=float32),\n",
              "  array([0.1], dtype=float32),\n",
              "  array([0.2], dtype=float32),\n",
              "  array([0.3], dtype=float32),\n",
              "  array([0.4], dtype=float32),\n",
              "  array([0.5], dtype=float32),\n",
              "  array([0.6], dtype=float32),\n",
              "  array([0.7], dtype=float32),\n",
              "  array([0.8], dtype=float32),\n",
              "  array([0.9], dtype=float32)],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Actor-Critic Implementation"
      ],
      "metadata": {
        "id": "f98kI1r8rtYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy (Actor) network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)       # smaller network (32 hidden units)\n",
        "        self.fc2 = nn.Linear(32, output_dim)      # output logits for each action\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''F.relu(self.fc1(x)) takes the output of the first layer\n",
        "        (which has 32 units) and applies the ReLU activation function to it.\n",
        "        This helps the network learn non-linear patterns.'''\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # Use softmax to get probabilities over actions\n",
        "\n",
        "        '''in this case the 32 nodes, undergo some liner vector combination,\n",
        "        which returns a vector size of output_dim, and the soft max thats the\n",
        "        output_dim to return a probability for each value of the output_dim\n",
        "        return probs  # tensor of shape [batch_size, output_dim] with probabilities'''\n",
        "\n",
        "        probs = F.softmax(self.fc2(x), dim=-1)\n",
        "# Value (Critic) network\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)  # outputs a single value\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        '''The Critic estimates the Value ($V(s)$) of the state, which represents\n",
        "        the total expected future reward'''\n",
        "\n",
        "        value = self.fc2(x)  # no activation on value output\n",
        "        return value         # tensor of shape [batch_size, 1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "8l-9K3DzrxmK",
        "outputId": "0c36da30-00b8-4c40-8708-9b30fa91ab93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2695328907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Policy (Actor) network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# smaller network (32 hidden units)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment, actor, critic, and optimizers\n",
        "env = DoubleGoalEnv()\n",
        "actor = PolicyNetwork(state_dim, action_dim)\n",
        "critic = ValueNetwork(state_dim)\n",
        "\n",
        "'''the optimizer is the \"manager.\" We tell the manager exactly which\n",
        "\"employees\" (parameters) to supervise (actor.parameters()) and how fast\n",
        "they should learn (lr or learning rate).'''\n",
        "\n",
        "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
        "\n",
        "'''Notice the Critic's learning rate (0.005) is higher than the Actor's\n",
        "(0.001). This is common; we often want the Critic to learn the value of\n",
        "states faster so it can provide accurate feedback to the Actor'''\n",
        "\n",
        "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=0.005)  # critic can have a different LR\n",
        "\n",
        "# Hyperparameters\n",
        "episodes = 200\n",
        "gamma = 0.99\n",
        "'''The usual policy gradient update rule is \"disguised\" in the code across three\n",
        "different steps.The Update ($\\theta_{t+1} = \\theta_t + \\dots$): This is handled\n",
        "by actor_optimizer.step().The Alpha ($\\alpha$): This is the lr=0.001 inside the\n",
        "optimizer.The Gradient ($\\nabla \\dots$): This is calculated automatically when\n",
        "we call .backward()'''\n",
        "\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    '''state is typically a single observation from your environment\n",
        "    (e.g., np.array([0.0]) or just 0.0); torch.tensor(...) converts\n",
        "    this state into a PyTorch tensor. For example, if state was\n",
        "    np.array([0.0]), it would become a tensor like tensor([0.]);\n",
        "    if the tensor had a shape of [1] after the first step, applying\n",
        "    .unsqueeze(0) changes its shape to [1, 1]\n",
        "\n",
        "    Even though we only feed one state at a time, PyTorch layers\n",
        "    (like nn.Linear) are strict. They always expect the input to\n",
        "    have a batch dimension: Input Shape = [Batch Size, Input Feature]\n",
        "    Since we only have a single state vector of shape [1], we use\n",
        "    .unsqueeze(0) to fake a batch dimension, turning it into [1, 1].'''\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    done = False\n",
        "    ep_return = 0.0  # track total reward in this episode\n",
        "    while not done:\n",
        "        # 1. Actor: get action probabilities and sample an action\n",
        "        probs = actor(state_tensor)            # probabilities over actions\n",
        "        # Sample action from the probability distribution\n",
        "        '''Categorical(probs) creates a statistical distribution based\n",
        "        on those probabilities'''\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        '''Using sample() introduces Exploration. If we used argmax\n",
        "        (always picking the highest probability) right from the start,\n",
        "        the agent would fall into a trap called a Local Optimum'''\n",
        "        action = action_dist.sample()          # draws an action index (0 or 1)\n",
        "        # Log probability of the chosen action (for loss calculation)\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "\n",
        "        # 2. Take the action in the environment\n",
        "        next_state, reward, done, _ = env.step(int(action.item()))\n",
        "        ep_return += reward\n",
        "\n",
        "        # 3. Critic: evaluate state and next state\n",
        "        '''The critic in this case is just standard TD(0) learning with function\n",
        "        approximation so it is v(s_t) = beta * (r_t+1 + gamma * v(s_t+1) - v(s_t)),\n",
        "        and we have a nn to approximate v(s), and the critic takes in the\n",
        "        prediction made by the actor in the form of the next step of the environment,\n",
        "        i.e. pi(a|s,theta); so the step(a) is influenced by the actor. Now step(a)\n",
        "        produces state s' which now the critic takes and evaluates what value to\n",
        "        assign the value function v(s')'''\n",
        "        value = critic(state_tensor)                             # V(s)\n",
        "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "        next_value = critic(next_state_tensor) if not done else torch.zeros(1, 1)\n",
        "\n",
        "        # 4. Compute the TD target and advantage\n",
        "        # TD target = r + γ * V(s') (or just r if done since next_value = 0)\n",
        "        td_target = torch.tensor([[reward]], dtype=torch.float32) + gamma * next_value\n",
        "        advantage = td_target - value  # (TD Error) how much better (positive) or worse (negative) the outcome was than expected\n",
        "\n",
        "        # 5. Compute critic loss (MSE between V(s) and TD target) and actor loss\n",
        "        critic_loss = F.mse_loss(value, td_target.detach())      # detach target to avoid affecting actor gradients\n",
        "        actor_loss = -log_prob * advantage.detach()              # policy gradient loss (detached advantage)\n",
        "\n",
        "        # 6. Backpropagate losses\n",
        "        actor_optimizer.zero_grad()\n",
        "        critic_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        critic_loss.backward()\n",
        "        '''This is a method belonging to a PyTorch optimizer object, its purpose\n",
        "        is to update the weights (parameters) of a neural network based on the\n",
        "        gradients that were computed during the backward pass (.backward()).'''\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "        # 7. Move to the next state\n",
        "        state_tensor = next_state_tensor\n",
        "\n",
        "    # Optionally print episode result\n",
        "    if episode % 20 == 0:\n",
        "        print(f\"Episode {episode}: Total Reward (Return) = {ep_return:.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "AR0pNido3kUM",
        "outputId": "043f1860-ac6c-4196-d84d-ee9a33a6b8e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DoubleGoalEnv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3884628100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize environment, actor, critic, and optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoubleGoalEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DoubleGoalEnv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Batch Size, Zeoring Gradients, Gradient Descent, SGD, Mini-Batch SGD\n",
        "\n",
        "We categorize these based on the **Batch Size** (how much data the model sees before updating weights).\n",
        "\n",
        "### **Batch Gradient Descent (Full Batch)**\n",
        "* **Batch Size:** $N$ (The entire dataset).\n",
        "* **Process:** The model calculates the error for *every* data point in the training set, averages them, and then performs a single weight update.\n",
        "* **Pros:** The trajectory is smooth and stable because it uses the true gradient of the dataset.\n",
        "* **Cons:** It is computationally very slow per step and memory-intensive. It can get stuck in \"saddle points\" (flat areas) because it lacks noise.\n",
        "* **Zero Grad:** Required between **Epochs** (passes through the dataset).\n",
        "\n",
        "### **Stochastic Gradient Descent (SGD)**\n",
        "* **Batch Size:** $1$ (A single data point).\n",
        "* **Process:** The model calculates the error and updates weights after *each* individual sample.\n",
        "* **Pros:** Updates are extremely fast. The \"noise\" from individual samples helps the model escape shallow local minima.\n",
        "* **Cons:** The path to the minimum is very jittery (high variance), which can make it hard to settle on the exact best solution.\n",
        "* **Zero Grad:** Required between **every sample**.\n",
        "\n",
        "### **Mini-Batch Gradient Descent**\n",
        "* **Batch Size:** $M$ (e.g., 32, 64, 128), where $1 < M < N$.\n",
        "* **Process:** The model updates weights after processing a small group of samples.\n",
        "* **Pros:** The \"Sweet Spot.\" It utilizes GPU parallelism efficiently (faster than SGD) and offers a balance of stability and useful noise (better than Full Batch).\n",
        "* **Cons:** Requires tuning an extra hyperparameter (the batch size).\n",
        "* **Zero Grad:** Required between **every batch**.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Why We Zero Out Gradients (`zero_grad()`)\n",
        "\n",
        "* **The Mechanism:** In PyTorch, calling `.backward()` **adds** (accumulates) the new gradients to the existing ones rather than replacing them.\n",
        "* **The Reason:** We generally want each update step to be based **only** on the error from the current batch (the current \"terrain\").\n",
        "* **The Rule:** If you do not zero out the gradients, you are mixing old directional instructions with new ones, leading to massive, incorrect updates. You must zero them out before every new `.step()` calculation."
      ],
      "metadata": {
        "id": "HWMiNlHyYrEt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEfQJ5HlaV_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}