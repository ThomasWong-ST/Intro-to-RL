{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTDS9V2vHGAK7+pS2i5iOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasWong-ST/Intro-to-RL/blob/main/Function_Approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wDQ6fV0_Fuj8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp 9.1: State Aggregation on the 1000-state Random Walk"
      ],
      "metadata": {
        "id": "psvdtkRhezDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CM9n3Y7XrEC3"
      },
      "outputs": [],
      "source": [
        "class RandomWalk:\n",
        "    \"\"\"\n",
        "    Implements the 1000-state random walk environment described in\n",
        "    Sutton and Barto's Reinforcement Learning, Example 9.1 (page 203).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.n_states = 1000\n",
        "        self.start_state = 500\n",
        "        self.step_range = 100\n",
        "\n",
        "        # Terminal states are outside the 1-1000 range.\n",
        "        # State 0 is the left terminal state, 1001 is the right.\n",
        "        self.left_terminal = 0\n",
        "        self.right_terminal = self.n_states + 1\n",
        "\n",
        "        self.terminal_rewards = (-1, 1)  # (left_reward, right_reward)\n",
        "        self.state = None\n",
        "\n",
        "        # Pre-calculate the possible moves for efficiency\n",
        "        moves = np.arange(1, self.step_range + 1)\n",
        "        self._possible_moves = np.concatenate((-moves, moves))\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Resets the environment to the starting state.\n",
        "        All episodes begin in state 500.\n",
        "        \"\"\"\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Takes a random step of 1-100 units to the left or right.\n",
        "\n",
        "        Returns:\n",
        "            next_state (int): The state after the move. Can be a terminal state.\n",
        "            reward (int): The reward for this transition.\n",
        "            done (bool): True if the episode has terminated.\n",
        "        \"\"\"\n",
        "        if self.state is None:\n",
        "            raise ValueError(\"You must call reset() before calling step()\")\n",
        "\n",
        "        # Select a random move from [-100, -1] U [1, 100]\n",
        "        move = np.random.choice(self._possible_moves)\n",
        "        next_state = self.state + move\n",
        "\n",
        "        # Check for termination\n",
        "        if next_state < 1:\n",
        "            reward = self.terminal_rewards[0]\n",
        "            done = True\n",
        "            self.state = self.left_terminal\n",
        "            return self.left_terminal, reward, done\n",
        "        elif next_state > self.n_states:\n",
        "            reward = self.terminal_rewards[1]\n",
        "            done = True\n",
        "            self.state = self.right_terminal\n",
        "            return self.right_terminal, reward, done\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "            self.state = next_state\n",
        "            return self.state, reward, done\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"Return list of nonterminal state indices.\"\"\"\n",
        "        return list(range(1, self.n_states + 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Weight Vector (w): We have a weight vector, let's call it w = [w₀, w₁, ..., w₉]ᵀ. It has 10 components, one for each group of states.\n",
        "\n",
        "2. Feature Vector (x(s)): For state aggregation, the feature vector x(s) is constructed in a special way. It's a vector that has:\n",
        "\n",
        "- The same number of components as the weight vector (10 in our case).\n",
        "\n",
        "- A value of 1 at the index corresponding to the group the state s belongs to.\n",
        "\n",
        "- A value of 0 for all other components.\n",
        "\n",
        "3. This is often called a one-hot encoding. For example:\n",
        "\n",
        "- If state s = 42 is in group 0, then x(42) = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]ᵀ.\n",
        "\n",
        "- If state s = 350 is in group 3, then x(350) = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]ᵀ.\n",
        "\n",
        "4. Calculating wᵀx(s): When we compute the dot product: wᵀx(s) = w₀*x₀(s) + w₁*x₁(s) + ... + w₉*x₉(s)\n",
        "\n",
        "Since only one component of x(s) is 1 (say, at index i) and the rest are 0, this simplifies to:\n",
        "\n",
        "wᵀx(s) = wᵢ * 1 = wᵢ\n",
        "\n",
        "...where i is the index returned by our get_group(s) function."
      ],
      "metadata": {
        "id": "q7G7gwruiDJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_group(state, num_groups=10, states_per_group=100):\n",
        "  \"\"\" Returns the index of the group the state belongs to. \"\"\"\n",
        "  # Ensure state is within valid range (1 to 1000 for non-terminals)\n",
        "  if state <= 0 or state > num_groups * states_per_group:\n",
        "        # Handle terminal or invalid states - they don't map to a feature\n",
        "        return None\n",
        "  return (state - 1) // states_per_group # group indices 0 to 9\n",
        "\n",
        "def v_hat_RW(state, w, num_groups=10, states_per_group=100):\n",
        "  \"\"\" Approximate value function using state aggregation. \"\"\"\n",
        "  group_index = get_group(state, num_groups, states_per_group)\n",
        "  if group_index is None:\n",
        "      return 0 # Value of terminal states is 0\n",
        "  return w[group_index]\n",
        "\n",
        "def grad_v_hat(state, num_weights=10, num_groups=10, states_per_group=100):\n",
        "  \"\"\" Gradient of the approximate value function for state aggregation. \"\"\"\n",
        "  group_index = get_group(state, num_groups, states_per_group)\n",
        "  gradient = np.zeros(num_weights)\n",
        "  if group_index is not None:\n",
        "      gradient[group_index] = 1.0\n",
        "  return gradient\n",
        "\n",
        "def generate_episdoe(env, policy, max_steps=100):\n",
        "  states, actions, rewards = [], [], []\n",
        "  state = env.reset()\n",
        "  for episode_num in range(max_steps):\n",
        "      # Generate an episode using the environment's dynamics\n",
        "      states, rewards = [], []\n",
        "      state = env.reset() # Starts at 500\n",
        "      done = False\n",
        "      while not done:\n",
        "            # In this random walk, there's no action choice by the policy.\n",
        "            # The environment dictates the next state.\n",
        "          next_state, reward, done = env.step()\n",
        "          states.append(state)\n",
        "          rewards.append(reward)\n",
        "          state = next_state\n",
        "          # Optional: Add a max_steps check here if needed\n",
        "      return states, actions, rewards"
      ],
      "metadata": {
        "id": "xet8W4cOF9JB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RandomWalk()\n",
        "dummy_policy = lambda s: None\n",
        "generated_episode = generate_episdoe(env, dummy_policy)\n",
        "print(generated_episode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVFBGONaDAYo",
        "outputId": "7d96576d-36ff-48ca-ca15-d5607116b502"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([500, np.int64(434), np.int64(354), np.int64(375), np.int64(405), np.int64(470), np.int64(538), np.int64(624), np.int64(555), np.int64(494), np.int64(556), np.int64(465), np.int64(460), np.int64(512), np.int64(416), np.int64(366), np.int64(354), np.int64(368), np.int64(359), np.int64(290), np.int64(345), np.int64(406), np.int64(335), np.int64(275), np.int64(204), np.int64(129), np.int64(207), np.int64(248), np.int64(340), np.int64(321), np.int64(260), np.int64(211), np.int64(266), np.int64(240), np.int64(312), np.int64(365), np.int64(462), np.int64(466), np.int64(454), np.int64(460), np.int64(538), np.int64(563), np.int64(485), np.int64(398), np.int64(437), np.int64(395), np.int64(320), np.int64(399), np.int64(443), np.int64(367), np.int64(400), np.int64(330), np.int64(396), np.int64(420), np.int64(410), np.int64(318), np.int64(328), np.int64(423), np.int64(489), np.int64(464), np.int64(513), np.int64(450), np.int64(397), np.int64(363), np.int64(388), np.int64(289), np.int64(251), np.int64(293), np.int64(196), np.int64(129), np.int64(196), np.int64(264), np.int64(340), np.int64(363), np.int64(373), np.int64(457), np.int64(508), np.int64(563), np.int64(660), np.int64(688), np.int64(603), np.int64(515), np.int64(502), np.int64(558), np.int64(653), np.int64(632), np.int64(563), np.int64(643), np.int64(705), np.int64(694), np.int64(789), np.int64(794), np.int64(747), np.int64(846), np.int64(937), np.int64(904), np.int64(852), np.int64(803), np.int64(822), np.int64(735), np.int64(703), np.int64(620), np.int64(621), np.int64(622), np.int64(584), np.int64(661), np.int64(585), np.int64(524), np.int64(532), np.int64(514), np.int64(545), np.int64(540), np.int64(490), np.int64(431), np.int64(422), np.int64(464), np.int64(453), np.int64(384), np.int64(437), np.int64(424), np.int64(483), np.int64(514), np.int64(484), np.int64(454), np.int64(370), np.int64(386), np.int64(294), np.int64(266), np.int64(340), np.int64(338), np.int64(294), np.int64(280), np.int64(277), np.int64(256), np.int64(190), np.int64(240), np.int64(300), np.int64(326), np.int64(349), np.int64(262), np.int64(224), np.int64(151), np.int64(82), np.int64(19)], [], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Monte Carlo Algorithm for Estimating $\\hat{v} ≈ v_{π}$\n",
        "\n",
        "In summary of what is happening with MC gradient. We define v hat and it's gradient, based on the structure of our example, it makes sense to choose a 10 element vector, where each element is a representation of an equal divid of the 1000 states (i.e. something like 0-100 for first element, 101 to 200 to second,...), and the gradient is just  $\\nabla\\hat{v}(s_t,\\mathbf{w}) = \\left( \\frac{\\partial\\hat{v}(s_t,\\mathbf{w})}{\\partial w_1}, \\frac{\\partial\\hat{v}(s_t,\\mathbf{w})}{\\partial w_2}, \\dots, \\frac{\\partial\\hat{v}(s_t,\\mathbf{w})}{\\partial w_d} \\right)^T$.\n",
        "\n",
        "We than have G_t which is obtained by standard MC return calculate with a whole episode, and than afterwards the weights are calculated using the update rule. My intuition here is that as the states are being explored, the agent will reach the terminal states and the reward of the terminal states will flow backward towards the starting position. and because the return for the first 5 element of v hat are bad, this naturally reduces the weights and the next 5 elements in v hat are good, so that naturally pushes the weights up.\n",
        "\n",
        "Note that this specific Gradient Monte Carlo algorithm is for prediction (evaluating states), not control (choosing actions). In the 1000-state random walk (Example 9.1), the \"agent\" doesn't actually choose its moves; the environment's step() function randomly moves it +/- 1 to 100 states. The algorithm learns the value of the states under this random movement policy. It doesn't learn a policy to prefer moving right."
      ],
      "metadata": {
        "id": "h-aIPzHag2vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_mc_prediction(env, policy, num_episodes=1000,max_steps=100, alpha=2e-5, gamma=1.0, num_weights=10):\n",
        "    \"\"\"\n",
        "    Implements Gradient Monte Carlo prediction (page 202).\n",
        "    Uses state aggregation from Example 9.1.\n",
        "    Note: 'policy' is not used here as the environment defines transitions,\n",
        "          but kept for consistency with your previous structure.\n",
        "    \"\"\"\n",
        "    w = np.zeros(num_weights) # Initialize weights\n",
        "\n",
        "    for episode_num in range(num_episodes):\n",
        "      states, actions, rewards = generate_episdoe(env, policy, max_steps)\n",
        "      G = 0\n",
        "      # Loop backwards through the episode steps\n",
        "      for t in reversed(range(len(states))):\n",
        "          G = gamma * G + rewards[t]\n",
        "          St = states[t]\n",
        "\n",
        "          # Calculate gradient and update weights\n",
        "          # No need for first-visit check with function approximation\n",
        "          gradient = grad_v_hat(St, num_weights)\n",
        "          delta = G - v_hat_RW(St, w) # Target G, prediction v_hat\n",
        "          w += alpha * delta * gradient # The core update rule (9.7)\n",
        "\n",
        "      # Optional: Add print statements to track progress, e.g., print RMS error\n",
        "      # if (episode_num + 1) % 100 == 0:\n",
        "      #     print(f\"Episode {episode_num+1} finished.\")\n",
        "\n",
        "\n",
        "    return w # Return the learned weights"
      ],
      "metadata": {
        "id": "G39UVQ4GCSEa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RandomWalk()\n",
        "\n",
        "# The policy argument isn't really needed for this specific env.step()\n",
        "# but we pass a dummy lambda function.\n",
        "dummy_policy = lambda s: None\n",
        "\n",
        "# Use alpha from the example figure caption [cite: 869]\n",
        "alpha_from_example = 2e-5\n",
        "\n",
        "learned_weights = gradient_mc_prediction(env, dummy_policy, num_episodes=30000, alpha=alpha_from_example)\n",
        "\n",
        "print(\"Learned Weights:\", learned_weights)\n",
        "\n",
        "# You can now use v_hat(state, learned_weights) to get predictions\n",
        "print(\"\\nExample Predictions:\")\n",
        "for s in [1, 50, 100, 500, 901, 950, 1000]:\n",
        "  print(f\"  State {s} (Group {get_group(s)}): Predicted Value = {v_hat_RW(s, learned_weights):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku05KxLPTX9x",
        "outputId": "6f687b24-b68b-406f-8650-f62aefac724f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Weights: [-0.63807779 -0.61957346 -0.46703318 -0.27897621 -0.08998633  0.08614611\n",
            "  0.27091379  0.45763158  0.62571118  0.64410345]\n",
            "\n",
            "Example Predictions:\n",
            "  State 1 (Group 0): Predicted Value = -0.6381\n",
            "  State 50 (Group 0): Predicted Value = -0.6381\n",
            "  State 100 (Group 0): Predicted Value = -0.6381\n",
            "  State 500 (Group 4): Predicted Value = -0.0900\n",
            "  State 901 (Group 9): Predicted Value = 0.6441\n",
            "  State 950 (Group 9): Predicted Value = 0.6441\n",
            "  State 1000 (Group 9): Predicted Value = 0.6441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semi Gradient TD(0) for Estimating $\\hat{v} ≈ v_{π}$"
      ],
      "metadata": {
        "id": "gljudbj8-Kpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semi_gradient_TD_zero(env, policy, num_episodes=1000,max_steps=500, alpha=2e-5, gamma=1.0, num_weights=10):\n",
        "  \"\"\"\n",
        "  For this specific RandomWalk environment, the step()\n",
        "  method internally randomizes the next state without needing\n",
        "  an action a as input. So, the policy argument to gradient_TD_zero\n",
        "  isn't actually used to choose actions.\n",
        "  \"\"\"\n",
        "  w = np.zeros(num_weights)\n",
        "  for ep in range(num_episodes):\n",
        "    s = env.reset()\n",
        "    #a = policy\n",
        "    steps = 0\n",
        "\n",
        "    while steps < max_steps:\n",
        "        s_next, r, done = env.step()\n",
        "\n",
        "        if done:\n",
        "            # terminal target: v_hat(S',w) = 0\n",
        "            td_target = r\n",
        "            w += alpha * (td_target - v_hat_RW(s, w))*grad_v_hat(s, num_weights)\n",
        "            steps += 1\n",
        "            break\n",
        "\n",
        "        # on-policy action for next state\n",
        "        #a_next = policy(Q, s_next)\n",
        "\n",
        "        # Q-learning target and update\n",
        "        #max_action = np.max(Q[s_next])\n",
        "        td_target = r + gamma * v_hat_RW(s_next, w) - v_hat_RW(s, w)\n",
        "        w += alpha * td_target * grad_v_hat(s, num_weights)\n",
        "\n",
        "        s = s_next\n",
        "        steps += 1\n",
        "  return w"
      ],
      "metadata": {
        "id": "fshDcIddD-Zu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RandomWalk()\n",
        "\n",
        "# The policy argument isn't really needed for this specific env.step()\n",
        "# but we pass a dummy lambda function.\n",
        "dummy_policy = lambda s: None\n",
        "\n",
        "# Use alpha from the example figure caption [cite: 869]\n",
        "alpha_from_example = 2e-5\n",
        "\n",
        "learned_weights = semi_gradient_TD_zero(env, policy=dummy_policy, num_episodes=10000, alpha=alpha_from_example)\n",
        "\n",
        "print(\"Learned Weights:\", learned_weights)\n",
        "\n",
        "# You can now use v_hat(state, learned_weights) to get predictions\n",
        "print(\"\\nExample Predictions:\")\n",
        "for s in [1, 50, 100, 500, 901, 950, 1000]:\n",
        "  print(f\"  State {s} (Group {get_group(s)}): Predicted Value = {v_hat_RW(s, learned_weights):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMNdNvHLJmyF",
        "outputId": "a8bbb78f-f11a-4a4b-b254-8f48b94fc74f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Weights: [-8.96420644e-02 -9.58753321e-03 -1.06855577e-03 -1.24668513e-04\n",
            " -1.29467460e-05  1.34778394e-05  1.24984884e-04  1.07660483e-03\n",
            "  9.41200760e-03  8.87900254e-02]\n",
            "\n",
            "Example Predictions:\n",
            "  State 1 (Group 0): Predicted Value = -0.0896\n",
            "  State 50 (Group 0): Predicted Value = -0.0896\n",
            "  State 100 (Group 0): Predicted Value = -0.0896\n",
            "  State 500 (Group 4): Predicted Value = -0.0000\n",
            "  State 901 (Group 9): Predicted Value = 0.0888\n",
            "  State 950 (Group 9): Predicted Value = 0.0888\n",
            "  State 1000 (Group 9): Predicted Value = 0.0888\n"
          ]
        }
      ]
    }
  ]
}